### abb 2/4/2020: looking for good imbalanced example
### https://www.kaggle.com/c/porto-seguro-safe-driver-prediction
### this approximates https://www.kaggle.com/abhilashawasthi/forza-baseline Private Score 0.28164 Public Score 0.27999
### see also: https://github.com/Zindear/Applied-Multivariate-Data-Analysis

# An imbalanced example, Kaggle Safe Drivers analysis

```{r setup}
#source('SafeDriver3.Rmd',echo=T,max.deparse.length=Inf)
setwd("C:/Users/bcraw/OneDrive/Documents/Barclay 2020/KaggleSafeDriver2017")
require(xgboost) # otherwise this is all base R, install.packages('xgboost')

basefile='SafeDriver3a' # just for keeping track of iterations
Sys.umask( '0002' )

dir.create( 'logs', showWarnings=F, recursive=T )
logfile <- file.path('logs', basefile)
sink( file=logfile, split=T )
print(Sys.time())


```

```{r loading the data}
temp <- tempfile()

#get https://raw.githubusercontent.com/brunocampos01/porto-seguro-safe-driver-prediction/master/data/raw/datasets.zip; unzip datasets.zip # read into R and save xz-compressed RData file. single file too big for github, splitting ...
system.time(print(load( 'SafeDriverTrain.RData' ))) # 1s, 12mb, sd0.train
system.time(print(load( 'SafeDriverTest.RData' ))) # 1s, 18mb, sd0.test, sd0.submission


x.train <- as.matrix( sd0.train[,-2,drop=F] ) # including id, can be predictive
y.train <- sd0.train[,2]

# create index for train/validation indicator
set.seed(1, "L'Ecuyer-CMRG") # reproducible, multicore

```

The data should be organized into different cohorts, and subsets should be left out for testing, validation, and cross-validation
```{r organizing the data}
# reduce for easier fitting at first
# keep <- sample(c(T,F), nrow(x.train), replace = T, prob = c(0.05, 0.95))

# for cross-validation
cohort <- sample(seq(11), size=nrow(x.train), replace = T, prob = c(rep(1, 10), 2))
# cohort==11 is the holdout sample for entire validation 

`%notin%` <- Negate(`%in%`)

training.x <- x.train[cohort %notin% c(1,11),]
training.y <- y.train[cohort %notin% c(1,11)]
reselect_ids <- sample(seq_along(training.y), size=10000, replace=T, prob=15*training.y+1)

d.train <- xgb.DMatrix(training.x[reselect_ids,], label = training.y[reselect_ids])
d.valid <- xgb.DMatrix(x.train[cohort==1,], label=y.train[cohort==1])
d.holdout <- xgb.DMatrix(x.train[cohort==11,], label=y.train[cohort==11])

watchlist <- list(train=d.train, valid=d.valid)
```


```{r designing a model}
ROC_AUC <- function(y_pred, y_train, plot_out = F) {
    ordering <- order(y_pred, decreasing=TRUE)
    y_true = getinfo(y_train, 'label')[ordering]
    y_pred <- y_pred[ordering]
    
    TPR <- cumsum(y_true)/sum(y_true)
    FPR <- cumsum(!y_true)/sum(!y_true)
    dFPR <- c(diff(FPR), 0)
    dTPR <- c(diff(TPR), 0)
    AUC <- sum(TPR * dFPR) + sum(dTPR * dFPR)/2 # final AUC
    if (plot_out) {
        #print(summary(cbind(TPR, FPR, dFPR, dTPR)))
        plot(FPR, TPR, main = paste0("AUC equal to: ", signif(AUC,5)))
    }
    return(list(metric="ROC_AUC", value=AUC))
}


weird_eval <- function(y_pred, y_train, plot_out=F, cutoff=0.5) {
  ordering <- order(y_pred, decreasing=T)
  n <- length(y_pred)
  
  y_pred <- sapply(y_pred[ordering], max, 0)
  y_true = getinfo(y_train, 'label')[ordering][ordering]
  
  get_score <- function(cutoff) {
    measure_a <- sapply(y_pred, max, cutoff) * !y_true
    measure_b <- sapply(y_pred, min, cutoff) * y_true
    overest <- cumsum(measure_a)
    underest <- sum(measure_b) - cumsum(measure_b)
    score <- (overest - underest)/n
    #print(data.frame(y_pred, y_true, measure_a, measure_b, overest, underest, score))
    score
  }
  scores <- get_score(cutoff = cutoff)
  
  if (plot_out) {
    par(mfcol=c(1,1))
    plot(y_pred, scores, type="l", ylim = c(-0.2, 1.2))
    points(y_pred, y_true, col="red")
  }
  
  range_preds <- max(y_pred) - min(y_pred) # penalize trees that don't bother making stronger predictions
  return(list(metric="weird_score", value=max(scores) * range_preds))
}


xgb_params <- list(eta=0.02, gamma=30, max_depth=6, subsample=0.4, objective="binary:logistic", min_child_weight=0.1)
mx1 <- xgb.train(params = xgb_params, data = d.train, nrounds=5000, feval=weird_eval, maximize = F, watchlist = watchlist, early_stopping_rounds = 100, print_every_n = 100)

# check against holdout
par(mfcol=c(1,2))
pred.y <- predict(mx1, d.holdout)
ROC_AUC(pred.y, d.holdout, plot_out=T)
rand.y <- runif(pred.y)
ROC_AUC(rand.y, d.holdout, plot_out = T)

fitted.y <- predict(mx1, d.train)
weird_eval(fitted.y, d.train, plot_out = T)
weird_eval(rand.y[1:length(fitted.y)], d.train, plot_out = T)
```








end