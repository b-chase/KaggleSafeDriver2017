<<<<<<< HEAD
knitr::opts_chunk$set(echo = TRUE)
library(xgboost)
knitr::opts_chunk$set(echo = TRUE)
require(xgboost)
install.packages("xgboost")
library(xgboost)
# load data
data(agaricus.train, package = 'xgboost')
data(agaricus.test, package = 'xgboost')
train <- agaricus.train
test <- agaricus.test
# fit model
bst <- xgboost(data = train$data, label = train$label, max_depth = 2, eta = 1, nrounds = 2,
nthread = 2, objective = "binary:logistic")
# predict
pred <- predict(bst, test$data)```
knitr::opts_chunk$set(echo = TRUE)
require(xgboost)
# load data
agaricus.test
# load data
data(agaricus.train, package = 'xgboost')
data(agaricus.test, package = 'xgboost')
train <- agaricus.train
test <- agaricus.test
train
# fit model
bst <- xgboost(data = train$data, label = train$label, max_depth = 2, eta = 1, nrounds = 2,nthread = 2, objective = "binary:logistic")
# predict
pred <- predict(bst, test$data)
pred
library(xgboost)
# load data
data(agaricus.train, package = 'xgboost')
data(agaricus.test, package = 'xgboost')
train <- agaricus.train
test <- agaricus.test
# fit model
bst <- xgboost(data = train$data, label = train$label, max_depth = 2, eta = 1, nrounds = 2,nthread = 2, objective = "binary:logistic")
# predict
pred <- predict(bst, test$data)
=======
require(xgboost) # otherwise this is all base R, install.packages('xgboost')
basefile='SafeDriver3a' # just for keeping track of iterations
Sys.umask( '0002' )
dir.create( 'logs', showWarnings=F, recursive=T )
logfile <- file.path('logs', basefile)
sink( file=logfile, split=T )
print(Sys.time())
#wget https://raw.githubusercontent.com/brunocampos01/porto-seguro-safe-driver-prediction/master/data/raw/datasets.zip; unzip datasets.zip # read into R and save xz-compressed RData file. single file too big for github, splitting ...
system.time(print(load( 'SafeDriverTrain.RData' ))) # 1s, 12mb, sd0.train
system.time(print(load( 'SafeDriverTest.RData' ))) # 1s, 18mb, sd0.test, sd0.submission
x.train <- as.matrix( sd0.train[,-2,drop=F] ) # including id, can be predictive
y.train <- sd0.train[,2]
# create index for train/validation indicator
set.seed(1, "L'Ecuyer-CMRG") # reproducible, multicore
prop.table(table(itrain <- sample( c(T,F), size=nrow(x.train), replace=T, prob=c(0.75,0.25) )))
#require(MLmetrics) # for NormalizedGini(), gini=2*AUCâ1, maps AUC to [0,1]
NormalizedGini <- function (y_pred, y_true) {
SumGini <- function(y_pred, y_true) {
y_true_sort <- as.numeric(y_true)[order(y_pred, decreasing=T)]
y_Lorentz <- cumsum(y_true_sort)/sum(y_true_sort)
y_random <- 1:length(y_pred)/length(y_pred)
return(sum(y_Lorentz - y_random))
}
return(NormalizedGini <- SumGini(y_pred, y_true)/SumGini(y_true, y_true))
}
NormalizedGiniXGB = function( y_pred, y_train ){
y_true = getinfo( y_train, 'label')
score = NormalizedGini( y_pred, y_true )
return(list( metric="ngini", value=score ))
}
# XGBoost
d1 <- xgb.DMatrix(x.train[itrain,], label=y.train[itrain]) # training frame
d2 <- xgb.DMatrix(x.train[!itrain,], label=y.train[!itrain]) # validation frame
#runlabel=paste0(basefile, '_run0')
# params from https://www.kaggle.com/abhilashawasthi/forza-baseline Private Score 0.28164 Public Score 0.27999
xgb.param1 <- list( eta=0.02, max_depth=4, subsample=0.9, colsample_bytree=0.9, objective='binary:logistic', seed=99, silent=T )
set.seed(1, "L'Ecuyer-CMRG") # reproducible, multicore
system.time(mx1 <- xgb.train( xgb.param1, d1, nrounds=5000, feval=NormalizedGiniXGB, maximize=T, watchlist=list(train=d1, valid=d2), early_stopping_rounds=100, print_every_n=100 )) #verbose_eval=100 ))
stopifnot(!is.na(l1 <- match( sd0.test$id, sd0.submission$id ))) # table(diff(l1))==1, just checking
sd0.submission$target[l1] <- predict( mx1, xgb.DMatrix(as.matrix(sd0.test)), ntree_limit=mx1$best_ntreelimit+50 )
write.csv( sd0.submission, paste0(runlabel,'.csv'), quote=F, row.names=F )
summary( sd0.submission )
>>>>>>> d965db375c764ee7f233ad9139dab008616eb2e9
