
> print(Sys.time())
[1] "2020-02-05 18:37:50 EST"

> #https://www.kaggle.com/captcalculator/r-xgboost-with-caret-tuning-and-gini-score
> # This is a minimal framework for training xgboost in R using caret to do the cross-validation/grid tuning
> # and using the normalized gini metric for scoring. The # of CV folds and size of the tuning grid
> # are limited to remain under kaggle kernel limits. To improve the score up the nrounds and expand
> # the tuning grid.
> #https://www.kaggle.com/kueipo/base-on-froza-pascal-single-xgb-lb-0-284
> #https://www.kaggle.com/nigelcarpenter/r-xgboost-with-gini-eval-and-stopping
> 
> library(data.table)

> library(caret)

> library(xgboost)

> # Read train and test data
> #dtrain <- fread('../input/train.csv')
> #dtest <- fread('../input/test.csv')
> system.time(print(load( 'SafeDriver.RData' ))) # 2s, 29mb, sd0.train, sd0.test
[1] "sd0.train"      "sd0.test"       "sd0.submission"
   user  system elapsed 
  3.108   0.140   3.252 

> # "Values of -1 indicate that the feature was missing" https://www.kaggle.com/c/porto-seguro-safe-driver-prediction/data
> min(sd0.train[,-1]) # -1
[1] -1

> tail(round(100*sort(sapply( sd0.train, function(x) sum(x==-1) ))/nrow(sd0.train),1),5)
ps_car_07_cat     ps_car_14     ps_reg_03 ps_car_05_cat ps_car_03_cat 
          1.9           7.2          18.1          44.8          69.1 

> tail(round(100*sort(sapply( sd0.test, function(x) sum(x==-1) ))/nrow(sd0.test),1),5)
ps_car_07_cat     ps_car_14     ps_reg_03 ps_car_05_cat ps_car_03_cat 
          1.9           7.1          18.1          44.8          69.1 

> #ps_car_07_cat     ps_car_14     ps_reg_03 ps_car_05_cat ps_car_03_cat 
> #          1.9           7.2          18.1          44.8          69.1
> #ps_car_07_cat     ps_car_14     ps_reg_03 ps_car_05_cat ps_car_03_cat 
> #          1.9           7.1          18.1          44.8          69.1
> # simple imputation with median:
> for( i in which(sapply( sd0.train, function(x) sum(x==-1) ) > 0) ) {
+     x <- sd0.train[[i]]
+     sd0.train[[i]][x == -1] <- median( x[x!=-1] )
+ }

> for( i in which(sapply( sd0.test, function(x) sum(x==-1) ) > 0) ) {
+     x <- sd0.test[[i]]
+     sd0.test[[i]][x == -1] <- median( x[x!=-1] )
+ }

> stopifnot( sd0.train[,-1] >= 0 )

> # abb1: drop _calc_* fields (per discussion, reason unknown?)
> dtrain <- data.table(sd0.train[, grep( '_calc_', colnames(sd0.train), value=T, invert=T )])

> dtest <- data.table(sd0.test[, grep( '_calc_', colnames(sd0.test), value=T, invert=T )])

> # collect names of all categorical variables
> (cat_vars <- names(dtrain)[grepl('_cat$', names(dtrain))])
 [1] "ps_ind_02_cat" "ps_ind_04_cat" "ps_ind_05_cat" "ps_car_01_cat" "ps_car_02_cat" "ps_car_03_cat" "ps_car_04_cat" "ps_car_05_cat" "ps_car_06_cat"
[10] "ps_car_07_cat" "ps_car_08_cat" "ps_car_09_cat" "ps_car_10_cat" "ps_car_11_cat"

> # turn categorical features into factors
> dtrain[, (cat_vars) := lapply(.SD, factor), .SDcols = cat_vars]

> dtest[, (cat_vars) := lapply(.SD, factor), .SDcols = cat_vars]

> # one hot encode the factor levels
> dtrain <- as.data.frame(model.matrix(~. - 1, data = dtrain))

> dtest <- as.data.frame(model.matrix(~ . - 1, data = dtest))

> # create index for train/test split
> set.seed(1, "L'Ecuyer-CMRG") # reproducible, multicore

> train_index <- sample(c(TRUE, FALSE), size = nrow(dtrain), replace = TRUE, prob = c(0.8, 0.2))

> # perform x/y ,train/test split.
> x_train <- dtrain[train_index, -(1:2)]

> y_train <- as.factor(dtrain$target[train_index])

> x_test <- dtrain[!train_index, 3:ncol(dtrain)]

> y_test <- as.factor(dtrain$target[!train_index])

> # Convert target factor levels to 0 = "No" and 1 = "Yes" to avoid this error when predicting class probs:
> # https://stackoverflow.com/questions/18402016/error-when-i-try-to-predict-class-probabilities-in-r-caret
> levels(y_train) <- c("No", "Yes")

> levels(y_test) <- c("No", "Yes")

> # normalized gini function taked from:
> # https://www.kaggle.com/c/ClaimPredictionChallenge/discussion/703
> normalizedGini <- function(aa, pp) {
+     Gini <- function(a, p) {
+         if (length(a) !=  length(p)) stop("Actual and Predicted need to be equal lengths!")
+         temp.df <- data.frame(actual = a, pred = p, range=c(1:length(a)))
+         temp.df <- temp.df[order(-temp.df$pred, temp.df$range),]
+         population.delta <- 1 / length(a)
+         total.losses <- sum(a)
+         null.losses <- rep(population.delta, length(a)) # Hopefully is similar to accumulatedPopulationPercentageSum
+         accum.losses <- temp.df$actual / total.losses # Hopefully is similar to accumulatedLossPercentageSum
+         gini.sum <- cumsum(accum.losses - null.losses) # Not sure if this is having the same effect or not
+         sum(gini.sum) / length(a)
+     }
+     Gini(aa,pp) / Gini(aa,aa)
+ }

> # create the normalized gini summary function to pass into caret
> giniSummary <- function (data, lev = "Yes", model = NULL) {
+     levels(data$obs) <- c('0', '1')
+     out <- normalizedGini(as.numeric(levels(data$obs))[data$obs], data[, lev[2]])  
+     names(out) <- "NormalizedGini"
+     out
+ }

> grep('xgb',names(getModelInfo()),ignore.case=T,value=T) # "xgbDART"   "xgbLinear" "xgbTree"
[1] "xgbDART"   "xgbLinear" "xgbTree"  

> grep('gbm',names(getModelInfo()),ignore.case=T,value=T) # "FH.GBML" "gbm_h2o" "gbm"
[1] "FH.GBML" "gbm_h2o" "gbm"    

> grep('bart',names(getModelInfo()),ignore.case=T,value=T) # "bartMachine"
[1] "bartMachine"

> paste( modelLookup("xgbTree")$parameter, collapse=', ' ) # nrounds, max_depth, eta, gamma, colsample_bytree, min_child_weight, subsample
[1] "nrounds, max_depth, eta, gamma, colsample_bytree, min_child_weight, subsample"

> # crudely estimate per-iteration tuning time with simplest setup + major assumptions.
> tc0 <- trainControl( number=3, method='cv', summaryFunction=giniSummary, allowParallel=T, classProbs=T, verboseIter=T )

> tg0 <- tg1 <- data.frame( nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 )

> print(itertime <- system.time(m1 <- train( x=x_train, y=y_train, method='xgbTree', trControl=tc0, tuneGrid=tg0, metric='NormalizedGini' )))
+ Fold1: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
- Fold1: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
+ Fold2: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
- Fold2: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
+ Fold3: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
- Fold3: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
Aggregating results
Fitting final model on full training set
   user  system elapsed 
773.306   4.623  85.306 

> (itertime=itertime['elapsed']/(tc0$number*tg0$nrounds)) # seconds per iter, per round
  elapsed 
0.5687067 

> # training control for tuning with grid search
> #xgboost: the seed object should be a list of length 11 with 10 integer vectors of size 36 and the last list element having at least a single integer set.seed(1); seeds <- sample(1e6,1e4) # reproducible
> tc2 <- trainControl( # instructions to caret
+     method='cv',
+     number=3, # folds/resamples, >5
+     summaryFunction=giniSummary,
+     allowParallel=T,
+     search='grid', # grid|random
+     classProbs=T,
+     sampling='smote',
+     verboseIter=T
+ )

> # theory: hyperparameters interact in clusters. set a few that are more independent.
> # clusters: 1. colsample_bytree+subsample+gamma, 2. eta+max_depth+min_child_weight+nrounds
> 
> if( F ) {
+ # 1. ballpark complexity first: max_depth+min_child_weight
+ tg <- expand.grid( max_depth=seq(3,9,by=3), min_child_weight=seq(1, 201, by=40) )
+ #m2a=m2; tg <- expand.grid( max_depth=4:7, min_child_weight=201 )
+ tg <- cbind( tg1[,setdiff(colnames(tg1),colnames(tg))], tg )
+ (itertime*tc2$number*sum(tg$nrounds))/(60) # estimated minutes
+ system.time(print(m2 <- train( x=x_train, y=y_train, method='xgbTree', trControl=tc2, tuneGrid=tg, metric='NormalizedGini' )))
+ m2$results[which.max(m2$results$NormalizedGini),c('max_depth','min_child_weight','NormalizedGini')]
+ #m2a$results[which.max(m2a$results$NormalizedGini),c('max_depth','min_child_weight','NormalizedGini')]
+ plot(m2) # clearly best: depth~5; not as important: minweight~120 (100-200, err lower)
+ tg1$max_depth=5; tg1$min_child_weight=120
+ 
+ # 2. ballpark over/under-fit bias-variance next: eta+nrounds
+ #nr=1e4; set.seed(1); i=sample(nrow(x_train), nr) # crude ranges from sample
+ #tg <- expand.grid( eta=seq(0.01,0.901,length=3), nrounds=seq(30, 1000, length=3) )
+ nr=nrow(x_train); i=1:nrow(x_train)
+ tg <- expand.grid( eta=seq(0.01,0.101,length=5), nrounds=500 )
+ tg <- cbind( tg1[,setdiff(colnames(tg1),colnames(tg))], tg )
+ (tg$min_child_weight <- tg$min_child_weight*(length(i)/nrow(x_train)))
+ ((nr/nrow(x_train))*itertime*tc2$number*sum(tg$nrounds[tg$nrounds==max(tg$nrounds)]))/(60) # estimated minutes
+ system.time(print(m2 <- train( x=x_train[i,], y=y_train[i], method='xgbTree', trControl=tc2, tuneGrid=tg, metric='NormalizedGini' )))
+ m2$results[which.max(m2$results$NormalizedGini),c('eta','nrounds','NormalizedGini')]
+ plot(m2) # clearly best: eta=0.03275
+ tg1$eta=0.03; tg1$nrounds=500
+ 
+ # 3. ballpark subsample+gamma (can't tune on colsample_by*?)
+ #https://www.kaggle.com/kueipo/base-on-froza-pascal-single-xgb-lb-0-284: 'colsample_bytree': 0.7, 'colsample_bylevel':0.7,
+ nr=nrow(x_train); i=1:nrow(x_train)
+ #tg <- expand.grid( nrounds=50, subsample=seq(0.5,1,length=3), gamma=seq(0,2,length=3) ) 
+ # probably want gamma=0, then increase if overfitting rears its ugly head.
+ tg <- expand.grid( nrounds=50, subsample=seq(0.1,0.5,length=5), gamma=0 )
+ tg <- cbind( tg1[,setdiff(colnames(tg1),colnames(tg))], tg )
+ tg$min_child_weight <- tg$min_child_weight*(length(i)/nrow(x_train))
+ ((nr/nrow(x_train))*itertime*tc2$number*sum(tg$nrounds[tg$nrounds==max(tg$nrounds)]))/(60) # estimated minutes
+ tg
+ system.time(print(m2 <- train( x=x_train[i,], y=y_train[i], method='xgbTree', trControl=tc2, tuneGrid=tg, metric='NormalizedGini' )))
+ plot(m2) # clearly best: subsample=0.3
+ tg1$subsample=0.3; tg1$gamma=0
+ 
+ }

> # critical for imbalanced datasets: split into 75/25 train/validation to check for overfit
> set.seed(1); i.validation <- sort(sample( nrow(dtrain), 0.25*nrow(dtrain) )); i.train <- (1:nrow(dtrain))[-i.validation]

> tg1$max_depth=5; tg1$min_child_weight=120

> tg1$eta=0.03; tg1$nrounds=500

> tg1$subsample=0.3; tg1$gamma=0

> # 4. optimize nrounds & gamma
> tc3 <- tc2

> tc3$number <- 10 # lots of folds

> tg <- expand.grid( nrounds=c(500,1000,1500), gamma=c(0,4,8,12,20,30) )

> tg <- cbind( tg1[,setdiff(colnames(tg1),colnames(tg))], tg )

> nr=length(i.train); i=i.train

> ((nr/nrow(x_train))*itertime*tc3$number*sum(tg$nrounds[tg$nrounds==max(tg$nrounds)]))/(60) # estimated minutes
 elapsed 
798.7651 

> tg$min_child_weight <- tg$min_child_weight*(length(i)/nrow(x_train))

> tg
   max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1          5 0.03         112.3623       0.3                1     500     0
2          5 0.03         112.3623       0.3                1    1000     0
3          5 0.03         112.3623       0.3                1    1500     0
4          5 0.03         112.3623       0.3                1     500     4
5          5 0.03         112.3623       0.3                1    1000     4
6          5 0.03         112.3623       0.3                1    1500     4
7          5 0.03         112.3623       0.3                1     500     8
8          5 0.03         112.3623       0.3                1    1000     8
9          5 0.03         112.3623       0.3                1    1500     8
10         5 0.03         112.3623       0.3                1     500    12
11         5 0.03         112.3623       0.3                1    1000    12
12         5 0.03         112.3623       0.3                1    1500    12
13         5 0.03         112.3623       0.3                1     500    20
14         5 0.03         112.3623       0.3                1    1000    20
15         5 0.03         112.3623       0.3                1    1500    20
16         5 0.03         112.3623       0.3                1     500    30
17         5 0.03         112.3623       0.3                1    1000    30
18         5 0.03         112.3623       0.3                1    1500    30

> #system.time(print(m3 <- train( x=x_train[i.train,], y=y_train[i.train], method='xgbTree', trControl=tc3, tuneGrid=tg, metric='NormalizedGini', print_every_n=100 )))
> system.time(print(m3 <- train( x=x_train[i.train,], y=y_train[i.train], method='xgbTree', trControl=tc3, tuneGrid=tg, metric='NormalizedGini', print_every_n=100, feval=Normalized_Gini_XGB, maximize=T, watchlist=list(train=x_train[i.train,]) )))
+ Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
 elapsed 
798.7651 
   max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1          5 0.03         112.3623       0.3                1     500     0
2          5 0.03         112.3623       0.3                1    1000     0
3          5 0.03         112.3623       0.3                1    1500     0
4          5 0.03         112.3623       0.3                1     500     4
5          5 0.03         112.3623       0.3                1    1000     4
6          5 0.03         112.3623       0.3                1    1500     4
7          5 0.03         112.3623       0.3                1     500     8
8          5 0.03         112.3623       0.3                1    1000     8
9          5 0.03         112.3623       0.3                1    1500     8
10         5 0.03         112.3623       0.3                1     500    12
11         5 0.03         112.3623       0.3                1    1000    12
12         5 0.03         112.3623       0.3                1    1500    12
13         5 0.03         112.3623       0.3                1     500    20
14         5 0.03         112.3623       0.3                1    1000    20
15         5 0.03         112.3623       0.3                1    1500    20
16         5 0.03         112.3623       0.3                1     500    30
17         5 0.03         112.3623       0.3                1    1000    30
18         5 0.03         112.3623       0.3                1    1500    30
+ Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in check.custom.eval() : object 'Normalized_Gini_XGB' not found
 
- Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in check.custom.eval() : object 'Normalized_Gini_XGB' not found
 
- Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in check.custom.eval() : object 'Normalized_Gini_XGB' not found
 
- Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in check.custom.eval() : object 'Normalized_Gini_XGB' not found
 
- Fold01: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in check.custom.eval() : object 'Normalized_Gini_XGB' not found
 
- Fold01: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in check.custom.eval() : object 'Normalized_Gini_XGB' not found
 
- Fold01: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold02: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in check.custom.eval() : object 'Normalized_Gini_XGB' not found
 
- Fold02: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
 elapsed 
798.7651 
   max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1          5 0.03         112.3623       0.3                1     500     0
2          5 0.03         112.3623       0.3                1    1000     0
3          5 0.03         112.3623       0.3                1    1500     0
4          5 0.03         112.3623       0.3                1     500     4
5          5 0.03         112.3623       0.3                1    1000     4
6          5 0.03         112.3623       0.3                1    1500     4
7          5 0.03         112.3623       0.3                1     500     8
8          5 0.03         112.3623       0.3                1    1000     8
9          5 0.03         112.3623       0.3                1    1500     8
10         5 0.03         112.3623       0.3                1     500    12
11         5 0.03         112.3623       0.3                1    1000    12
12         5 0.03         112.3623       0.3                1    1500    12
13         5 0.03         112.3623       0.3                1     500    20
14         5 0.03         112.3623       0.3                1    1000    20
15         5 0.03         112.3623       0.3                1    1500    20
16         5 0.03         112.3623       0.3                1     500    30
17         5 0.03         112.3623       0.3                1    1000    30
18         5 0.03         112.3623       0.3                1    1500    30
+ Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgboost::xgb.train(list(eta = param$eta, max_depth = param$max_depth,  : 
  watchlist must be a list of xgb.DMatrix elements
 
- Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgboost::xgb.train(list(eta = param$eta, max_depth = param$max_depth,  : 
  watchlist must be a list of xgb.DMatrix elements
 
- Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgboost::xgb.train(list(eta = param$eta, max_depth = param$max_depth,  : 
  watchlist must be a list of xgb.DMatrix elements
 
- Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgboost::xgb.train(list(eta = param$eta, max_depth = param$max_depth,  : 
  watchlist must be a list of xgb.DMatrix elements
 
- Fold01: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgboost::xgb.train(list(eta = param$eta, max_depth = param$max_depth,  : 
  watchlist must be a list of xgb.DMatrix elements
 
+ Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.DMatrix(x_train[i.train, ]) : 
  xgb.DMatrix does not support construction from list
 
- Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.DMatrix(x_train[i.train, ]) : 
  xgb.DMatrix does not support construction from list
 
- Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.000500 
[101]	train-NormalizedGini:0.006924 
[201]	train-NormalizedGini:0.027285 
[301]	train-NormalizedGini:0.026971 
[401]	train-NormalizedGini:0.013691 
[501]	train-NormalizedGini:0.008404 
[601]	train-NormalizedGini:0.011598 
[701]	train-NormalizedGini:0.008630 
[801]	train-NormalizedGini:-0.001451 
[901]	train-NormalizedGini:-0.000513 
[1001]	train-NormalizedGini:-0.004710 
[1101]	train-NormalizedGini:-0.006666 
[1201]	train-NormalizedGini:-0.005201 
[1301]	train-NormalizedGini:-0.006140 
[1401]	train-NormalizedGini:-0.009144 
[1500]	train-NormalizedGini:-0.007660 
- Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.000290 
[101]	train-NormalizedGini:0.016117 
[201]	train-NormalizedGini:0.014622 
[301]	train-NormalizedGini:0.016357 
[401]	train-NormalizedGini:0.010883 
[501]	train-NormalizedGini:0.014177 
[601]	train-NormalizedGini:0.024117 
[701]	train-NormalizedGini:0.020711 
[801]	train-NormalizedGini:0.036508 
[901]	train-NormalizedGini:0.029658 
[1001]	train-NormalizedGini:0.030707 
[1101]	train-NormalizedGini:0.031231 
[1201]	train-NormalizedGini:0.029339 
[1301]	train-NormalizedGini:0.035879 
[1401]	train-NormalizedGini:0.043836 
[1500]	train-NormalizedGini:0.043764 
- Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.065637 
[101]	train-NormalizedGini:0.049885 
[201]	train-NormalizedGini:0.063220 
[301]	train-NormalizedGini:0.051997 
[401]	train-NormalizedGini:0.041962 
[501]	train-NormalizedGini:0.049240 
[601]	train-NormalizedGini:0.045171 
[701]	train-NormalizedGini:0.047571 
[801]	train-NormalizedGini:0.041744 
[901]	train-NormalizedGini:0.042946 
[1001]	train-NormalizedGini:0.043597 
[1101]	train-NormalizedGini:0.045190 
[1201]	train-NormalizedGini:0.043193 
[1301]	train-NormalizedGini:0.038346 
[1401]	train-NormalizedGini:0.032378 
[1500]	train-NormalizedGini:0.029646 
- Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.065544 
[101]	train-NormalizedGini:0.025618 
[201]	train-NormalizedGini:0.013708 
[301]	train-NormalizedGini:0.030749 
[401]	train-NormalizedGini:0.027930 
[501]	train-NormalizedGini:0.034864 
[601]	train-NormalizedGini:0.041575 
[701]	train-NormalizedGini:0.040203 
[801]	train-NormalizedGini:0.037392 
[901]	train-NormalizedGini:0.032527 
[1001]	train-NormalizedGini:0.033173 
[1101]	train-NormalizedGini:0.029232 
[1201]	train-NormalizedGini:0.026105 
[1301]	train-NormalizedGini:0.023104 
[1401]	train-NormalizedGini:0.021316 
[1500]	train-NormalizedGini:0.013376 
- Fold01: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.002323 
[101]	train-NormalizedGini:0.044035 
[201]	train-NormalizedGini:0.047385 
[301]	train-NormalizedGini:0.050184 
[401]	train-NormalizedGini:0.049986 
[501]	train-NormalizedGini:0.047446 
[601]	train-NormalizedGini:0.051402 
[701]	train-NormalizedGini:0.049045 
[801]	train-NormalizedGini:0.048597 
[901]	train-NormalizedGini:0.048516 
[1001]	train-NormalizedGini:0.049501 
[1101]	train-NormalizedGini:0.049590 
[1201]	train-NormalizedGini:0.049095 
[1301]	train-NormalizedGini:0.049094 
[1401]	train-NormalizedGini:0.050671 
[1500]	train-NormalizedGini:0.049707 
- Fold01: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.002224 
[101]	train-NormalizedGini:0.058062 
[201]	train-NormalizedGini:0.064231 
[301]	train-NormalizedGini:0.060229 
[401]	train-NormalizedGini:0.062774 
[501]	train-NormalizedGini:0.062095 
[601]	train-NormalizedGini:0.063245 
[701]	train-NormalizedGini:0.063124 
[801]	train-NormalizedGini:0.063212 
[901]	train-NormalizedGini:0.063284 
[1001]	train-NormalizedGini:0.063428 
[1101]	train-NormalizedGini:0.063173 
[1201]	train-NormalizedGini:0.063164 
[1301]	train-NormalizedGini:0.062975 
[1401]	train-NormalizedGini:0.064509 
[1500]	train-NormalizedGini:0.063314 
- Fold01: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold02: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:44:47] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold02: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold02: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:44:49] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold02: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold02: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:44:52] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold02: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold02: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:44:54] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold02: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold02: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:44:56] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold02: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold02: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:44:59] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold02: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold03: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:01] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold03: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold03: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:03] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold03: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold03: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:05] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold03: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold03: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:08] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold03: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold03: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:10] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold03: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold03: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:12] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold03: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold04: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:14] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold04: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold04: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:16] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold04: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold04: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:19] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold04: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold04: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:21] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold04: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold04: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:23] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold04: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold04: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:25] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold04: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold05: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:28] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold05: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold05: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:30] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold05: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold05: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:32] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold05: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold05: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:34] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold05: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold05: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:36] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold05: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold05: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:39] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold05: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold06: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:41] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold06: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold06: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:43] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold06: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold06: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:45] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold06: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold06: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:47] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold06: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold06: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:49] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold06: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold06: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:51] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold06: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold07: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:53] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold07: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold07: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:56] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold07: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold07: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:45:58] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold07: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold07: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:01] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold07: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold07: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:03] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold07: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold07: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:05] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold07: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold08: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:07] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold08: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold08: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:09] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold08: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold08: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:11] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold08: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold08: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:13] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold08: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold08: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:16] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold08: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold08: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:18] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold08: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold09: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:20] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold09: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold09: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:23] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold09: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold09: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:25] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold09: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold09: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:27] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold09: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold09: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:30] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold09: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold09: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:32] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold09: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold10: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:34] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold10: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold10: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:37] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold10: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold10: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:39] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold10: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold10: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:41] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold10: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold10: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:44] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold10: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold10: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:46] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold10: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold11: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold11: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:48] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold11: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold11: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold11: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:50] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold11: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold11: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold11: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:53] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold11: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold11: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold11: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:54] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold11: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold11: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold11: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:57] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold11: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
+ Fold11: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
model fit failed for Fold11: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [19:46:59] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold11: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=112.4, subsample=0.3, nrounds=1500 
Aggregating results
Selecting tuning parameters
Fitting nrounds = 500, max_depth = 5, eta = 0.03, gamma = 0, colsample_bytree = 1, min_child_weight = 112, subsample = 0.3 on full training set
 Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 2 1 1 1 ...
 num [1:476753] 1 1 1 1 1 1 2 1 1 1 ...
 num [1:476753] 0 0 0 0 0 0 1 0 0 0 ...
   max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1          5 0.03         112.3623       0.3                1     500     0
2          5 0.03         112.3623       0.3                1    1000     0
3          5 0.03         112.3623       0.3                1    1500     0
4          5 0.03         112.3623       0.3                1     500     4
5          5 0.03         112.3623       0.3                1    1000     4
6          5 0.03         112.3623       0.3                1    1500     4
7          5 0.03         112.3623       0.3                1     500     8
8          5 0.03         112.3623       0.3                1    1000     8
9          5 0.03         112.3623       0.3                1    1500     8
10         5 0.03         112.3623       0.3                1     500    12
11         5 0.03         112.3623       0.3                1    1000    12
12         5 0.03         112.3623       0.3                1    1500    12
13         5 0.03         112.3623       0.3                1     500    20
14         5 0.03         112.3623       0.3                1    1000    20
15         5 0.03         112.3623       0.3                1    1500    20
16         5 0.03         112.3623       0.3                1     500    30
17         5 0.03         112.3623       0.3                1    1000    30
18         5 0.03         112.3623       0.3                1    1500    30
 elapsed 
798.7651 
   max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1          5 0.03         112.3623       0.3                1     500     0
2          5 0.03         112.3623       0.3                1    1000     0
3          5 0.03         112.3623       0.3                1    1500     0
4          5 0.03         112.3623       0.3                1     500     4
5          5 0.03         112.3623       0.3                1    1000     4
6          5 0.03         112.3623       0.3                1    1500     4
7          5 0.03         112.3623       0.3                1     500     8
8          5 0.03         112.3623       0.3                1    1000     8
9          5 0.03         112.3623       0.3                1    1500     8
10         5 0.03         112.3623       0.3                1     500    12
11         5 0.03         112.3623       0.3                1    1000    12
12         5 0.03         112.3623       0.3                1    1500    12
13         5 0.03         112.3623       0.3                1     500    20
14         5 0.03         112.3623       0.3                1    1000    20
15         5 0.03         112.3623       0.3                1    1500    20
16         5 0.03         112.3623       0.3                1     500    30
17         5 0.03         112.3623       0.3                1    1000    30
18         5 0.03         112.3623       0.3                1    1500    30
 num [1:595212] 0 0 0 0 0 0 0 0 0 1 ...
   elapsed 
0.08875168 
  max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1         5 0.03         112.3623       0.3                1      10     0
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
[1]	train-NormalizedGini:-0.007907 
[10]	train-NormalizedGini:0.045033 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:02:25] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
[1]	train-NormalizedGini:0.057381 
[10]	train-NormalizedGini:0.021973 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:03:53] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:05:43] amalgamation/../src/objective/regression_obj.cu:65: Check failed: preds.Size() == info.labels_.Size() (1072341 vs. 357447) : labels are not correctly providedpreds.size=1072341, label.size=357447
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x227) [0x7f453dcb47d7]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpd
 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:05:44] amalgamation/../src/objective/regression_obj.cu:65: Check failed: preds.Size() == info.labels_.Size() (266886 vs. 88962) : labels are not correctly providedpreds.size=266886, label.size=88962
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x227) [0x7f453dcb47d7]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateO
 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:06:39] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
 Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 2 1 1 1 ...
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgboost::xgb.DMatrix(x, label = y, missing = NA) : 
  REAL() can only be applied to a 'numeric', not a 'logical'
 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
eXtreme Gradient Boosting 

476753 samples
   185 predictor
     2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Cross-Validated (1 fold) 
Summary of sample sizes: 0 
Resampling results:

  NormalizedGini
  0.002801965   

Tuning parameter 'nrounds' was held constant at a value of 10
Tuning parameter 'max_depth' was held constant at a value of 5
Tuning parameter
 held constant at a value of 1
Tuning parameter 'min_child_weight' was held constant at a value of 112.3623
Tuning parameter 'subsample' was
 held constant at a value of 0.3
   user  system elapsed 
 49.859   0.601   7.983 
+ Fold1: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
- Fold1: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
+ Fold2: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
- Fold2: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
+ Fold3: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
- Fold3: nrounds=50, max_depth=6, eta=0.3, gamma=0.1, min_child_weight=5, subsample=1, colsample_bytree=1 
Aggregating results
Fitting final model on full training set
   user  system elapsed 
744.538   3.328  79.285 
      user     system    elapsed 
348.575115   1.558091  37.119365 
  max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1         5 0.03         112.3623       0.3                1      10     0
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:10:19] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:10:21] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold4: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold4: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:10:23] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold4: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
eXtreme Gradient Boosting 

476753 samples
   185 predictor
     2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 317835, 317835, 317836 
Resampling results:

  NormalizedGini
  0.2269465     

Tuning parameter 'nrounds' was held constant at a value of 10
Tuning parameter 'max_depth' was held constant at a value of 5
Tuning parameter
 held constant at a value of 1
Tuning parameter 'min_child_weight' was held constant at a value of 112.3623
Tuning parameter 'subsample' was
 held constant at a value of 0.3
   user  system elapsed 
155.858   2.997  27.881 
eXtreme Gradient Boosting 

476753 samples
   185 predictor
     2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 317835, 317835, 317836 
Resampling results:

  NormalizedGini
  0.2269465     

Tuning parameter 'nrounds' was held constant at a value of 10
Tuning parameter 'max_depth' was held constant at a value of 5
Tuning parameter
 held constant at a value of 1
Tuning parameter 'min_child_weight' was held constant at a value of 112.3623
Tuning parameter 'subsample' was
 held constant at a value of 0.3
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
eXtreme Gradient Boosting 

476753 samples
   185 predictor
     2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 317835, 317835, 317836 
Resampling results:

  NormalizedGini
  0.2239548     

Tuning parameter 'nrounds' was held constant at a value of 10
Tuning parameter 'max_depth' was held constant at a value of 5
Tuning parameter
 held constant at a value of 1
Tuning parameter 'min_child_weight' was held constant at a value of 112.3623
Tuning parameter 'subsample' was
 held constant at a value of 0.3
   user  system elapsed 
157.055   3.045  28.145 
 Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 2 1 1 1 ...
 Factor w/ 2 levels "No","Yes": 1 1 1 1 1 1 2 1 1 1 ...
'data.frame':	446409 obs. of  185 variables:
 $ ps_ind_01       : num  2 1 5 2 5 5 1 2 1 5 ...
 $ ps_ind_02_cat1  : num  0 1 1 1 1 1 1 1 1 1 ...
 $ ps_ind_02_cat2  : num  1 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_02_cat3  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_02_cat4  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_03       : num  5 7 4 3 4 3 2 1 3 11 ...
 $ ps_ind_04_cat1  : num  1 0 0 1 0 1 0 1 1 0 ...
 $ ps_ind_05_cat1  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_05_cat2  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_05_cat3  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_05_cat4  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_05_cat5  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_05_cat6  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_06_bin   : num  0 0 0 0 1 0 0 0 0 0 ...
 $ ps_ind_07_bin   : num  1 0 0 1 0 0 1 0 1 0 ...
 $ ps_ind_08_bin   : num  0 1 0 0 0 1 0 1 0 0 ...
 $ ps_ind_09_bin   : num  0 0 1 0 0 0 0 0 0 1 ...
 $ ps_ind_10_bin   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_11_bin   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_12_bin   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_13_bin   : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_14       : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_15       : num  11 3 6 8 13 6 4 10 12 10 ...
 $ ps_ind_16_bin   : num  0 0 1 1 1 1 0 1 1 0 ...
 $ ps_ind_17_bin   : num  1 0 0 0 0 0 0 0 0 0 ...
 $ ps_ind_18_bin   : num  0 1 0 0 0 0 1 0 0 1 ...
 $ ps_reg_01       : num  0.7 0.8 0.9 0.6 0.7 0.9 0.9 0.5 0.7 0.8 ...
 $ ps_reg_02       : num  0.2 0.4 1.8 0.1 0.4 0.7 1.4 0.2 0.9 0.6 ...
 $ ps_reg_03       : num  0.718 0.766 2.333 0.617 0.607 ...
 $ ps_car_01_cat1  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_01_cat2  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_01_cat3  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_01_cat4  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_01_cat5  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_01_cat6  : num  0 0 0 1 0 0 0 1 0 0 ...
 $ ps_car_01_cat7  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_01_cat8  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_01_cat9  : num  0 0 0 0 0 0 0 0 1 0 ...
 $ ps_car_01_cat10 : num  1 0 1 0 0 1 0 0 0 0 ...
 $ ps_car_01_cat11 : num  0 1 0 0 1 0 1 0 0 1 ...
 $ ps_car_02_cat1  : num  1 1 0 1 1 1 0 1 1 1 ...
 $ ps_car_03_cat1  : num  1 1 1 1 0 1 0 1 0 1 ...
 $ ps_car_04_cat1  : num  0 0 0 0 0 0 1 0 0 0 ...
 $ ps_car_04_cat2  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_04_cat3  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_04_cat4  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_04_cat5  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_04_cat6  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_04_cat7  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_04_cat8  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_04_cat9  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_05_cat1  : num  1 1 0 1 0 1 0 1 1 1 ...
 $ ps_car_06_cat1  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat2  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat3  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat4  : num  1 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat5  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat6  : num  0 0 0 0 0 0 0 0 1 0 ...
 $ ps_car_06_cat7  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat8  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat9  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat10 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat11 : num  0 1 0 1 1 0 0 1 0 1 ...
 $ ps_car_06_cat12 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat13 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat14 : num  0 0 1 0 0 1 1 0 0 0 ...
 $ ps_car_06_cat15 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat16 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_06_cat17 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_07_cat1  : num  1 1 1 1 1 1 1 1 1 1 ...
 $ ps_car_08_cat1  : num  0 1 1 1 1 1 1 1 1 1 ...
 $ ps_car_09_cat1  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_09_cat2  : num  0 1 0 0 1 0 1 1 1 1 ...
 $ ps_car_09_cat3  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_09_cat4  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_10_cat1  : num  1 1 1 1 1 1 1 1 1 1 ...
 $ ps_car_10_cat2  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat2  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat3  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat4  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat5  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat6  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat7  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat8  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat9  : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat10 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat11 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat12 : num  1 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat13 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat14 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat15 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat16 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat17 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat18 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat19 : num  0 1 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat20 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat21 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat22 : num  0 0 0 0 0 0 0 0 0 0 ...
 $ ps_car_11_cat23 : num  0 0 0 0 0 0 0 0 0 0 ...
  [list output truncated]
[1] 446409    185
[1] 476753    185
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:14:36] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:14:38] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold4: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold4: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:14:40] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold4: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
[1] 1 2 3 4 5 6
[1] 595203 595206 595207 595208 595210 595211
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:15:33] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:15:35] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold4: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold4: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in xgb.iter.update(bst$handle, dtrain, iteration - 1, obj) : 
  [20:15:37] amalgamation/../src/objective/regression_obj.cu:101: label must be in [0,1] for logistic regression
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::obj::RegLossObj<xgboost::obj::LogisticClassification>::GetGradient(xgboost::HostDeviceVector<float> const&, xgboost::MetaInfo const&, int, xgboost::HostDeviceVector<xgboost::detail::GradientPairInternal<float> >*)+0x3a6) [0x7f453dcb4956]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::LearnerImpl::UpdateOneIter(int, xgboost::DMatrix*)+0x289) [0x7f453dcd64f9]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter+0x45) [0x7f453dc6e7b5]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGBoosterUpdateOneIter_R+0x3c) [0x7f453dc4bbcc]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44
 
- Fold4: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
   user  system elapsed 
  1.809   0.008   0.664 
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
   user  system elapsed 
 27.534   0.003   4.876 
[1]      1 595211
[1] 476753    185
[1] 595212    187
+ Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold1: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold2: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
- Fold3: max_depth=5, eta=0.03, min_child_weight=112.4, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
   user  system elapsed 
154.840   3.108  27.881 
    user   system  elapsed 
372.2690   1.6640  39.6425 
  max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1         5 0.03              120       0.3                1      10     0
+ Fold1: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold1: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in slice.xgb.DMatrix(object, idxset) : 
  [20:19:58] ./include/xgboost/./../../src/common/span.h:403: Check failed: _count >= 0: 
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::common::Span<xgboost::Entry const, -1l>::Span(xgboost::Entry const*, long)+0xe2) [0x7f453dc8e182]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::SparsePage::operator[](unsigned long) const+0x64) [0x7f453dc8e224]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGDMatrixSliceDMatrix+0x2e8) [0x7f453dc6d878]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGDMatrixSliceDMatrix_R+0xe4) [0x7f453dc4b3e4]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44]
  [bt] (6) /usr/lib/R/lib/libR.so(Rf_eval+0x190) [0x7f4549bdfc80]
  [bt] (7) /usr/lib/R/lib/libR.so(+0x13cadf) [0x7f4549be1adf]
  [bt] (8) /usr/lib/R/lib/libR.so(Rf_apply
 
- Fold1: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold2: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in slice.xgb.DMatrix(object, idxset) : 
  [20:20:01] ./include/xgboost/./../../src/common/span.h:403: Check failed: _count >= 0: 
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::common::Span<xgboost::Entry const, -1l>::Span(xgboost::Entry const*, long)+0xe2) [0x7f453dc8e182]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::SparsePage::operator[](unsigned long) const+0x64) [0x7f453dc8e224]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGDMatrixSliceDMatrix+0x2e8) [0x7f453dc6d878]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGDMatrixSliceDMatrix_R+0xe4) [0x7f453dc4b3e4]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44]
  [bt] (6) /usr/lib/R/lib/libR.so(Rf_eval+0x190) [0x7f4549bdfc80]
  [bt] (7) /usr/lib/R/lib/libR.so(+0x13cadf) [0x7f4549be1adf]
  [bt] (8) /usr/lib/R/lib/libR.so(Rf_apply
 
- Fold2: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold3: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in slice.xgb.DMatrix(object, idxset) : 
  [20:20:03] ./include/xgboost/./../../src/common/span.h:403: Check failed: _count >= 0: 
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::common::Span<xgboost::Entry const, -1l>::Span(xgboost::Entry const*, long)+0xe2) [0x7f453dc8e182]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::SparsePage::operator[](unsigned long) const+0x64) [0x7f453dc8e224]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGDMatrixSliceDMatrix+0x2e8) [0x7f453dc6d878]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGDMatrixSliceDMatrix_R+0xe4) [0x7f453dc4b3e4]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44]
  [bt] (6) /usr/lib/R/lib/libR.so(Rf_eval+0x190) [0x7f4549bdfc80]
  [bt] (7) /usr/lib/R/lib/libR.so(+0x13cadf) [0x7f4549be1adf]
  [bt] (8) /usr/lib/R/lib/libR.so(Rf_apply
 
- Fold3: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
    user   system  elapsed 
372.2690   1.6640  39.6425 
  max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1         5 0.03              120       0.3                1      10     0
    user   system  elapsed 
372.2690   1.6640  39.6425 
  elapsed 
0.5285667 
  elapsed 
0.2642833 
  max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1         5 0.03              120       0.3                1      10     0
+ Fold1: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold1: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in slice.xgb.DMatrix(object, idxset) : 
  [20:22:50] ./include/xgboost/./../../src/common/span.h:403: Check failed: _count >= 0: 
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::common::Span<xgboost::Entry const, -1l>::Span(xgboost::Entry const*, long)+0xe2) [0x7f453dc8e182]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::SparsePage::operator[](unsigned long) const+0x64) [0x7f453dc8e224]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGDMatrixSliceDMatrix+0x2e8) [0x7f453dc6d878]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGDMatrixSliceDMatrix_R+0xe4) [0x7f453dc4b3e4]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44]
  [bt] (6) /usr/lib/R/lib/libR.so(Rf_eval+0x190) [0x7f4549bdfc80]
  [bt] (7) /usr/lib/R/lib/libR.so(+0x13cadf) [0x7f4549be1adf]
  [bt] (8) /usr/lib/R/lib/libR.so(Rf_apply
 
- Fold1: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
model fit failed for Fold2: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 Error in slice.xgb.DMatrix(object, idxset) : 
  [20:22:52] ./include/xgboost/./../../src/common/span.h:403: Check failed: _count >= 0: 
Stack trace:
  [bt] (0) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(dmlc::LogMessageFatal::~LogMessageFatal()+0x43) [0x7f453dc50613]
  [bt] (1) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::common::Span<xgboost::Entry const, -1l>::Span(xgboost::Entry const*, long)+0xe2) [0x7f453dc8e182]
  [bt] (2) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(xgboost::SparsePage::operator[](unsigned long) const+0x64) [0x7f453dc8e224]
  [bt] (3) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGDMatrixSliceDMatrix+0x2e8) [0x7f453dc6d878]
  [bt] (4) /usr/local/lib/R/site-library/xgboost/libs/xgboost.so(XGDMatrixSliceDMatrix_R+0xe4) [0x7f453dc4b3e4]
  [bt] (5) /usr/lib/R/lib/libR.so(+0x130f44) [0x7f4549bd5f44]
  [bt] (6) /usr/lib/R/lib/libR.so(Rf_eval+0x190) [0x7f4549bdfc80]
  [bt] (7) /usr/lib/R/lib/libR.so(+0x13cadf) [0x7f4549be1adf]
  [bt] (8) /usr/lib/R/lib/libR.so(Rf_apply
 
- Fold2: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold1: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
[1]	train-NormalizedGini:-0.002364 
[2]	train-NormalizedGini:0.040426 
[3]	train-NormalizedGini:-0.024092 
[4]	train-NormalizedGini:0.007862 
[5]	train-NormalizedGini:0.008440 
[6]	train-NormalizedGini:-0.013405 
[7]	train-NormalizedGini:-0.014247 
[8]	train-NormalizedGini:-0.013727 
[9]	train-NormalizedGini:-0.013992 
[10]	train-NormalizedGini:-0.004052 
- Fold1: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
[1]	train-NormalizedGini:0.070428 
[2]	train-NormalizedGini:0.073965 
[3]	train-NormalizedGini:0.074976 
[4]	train-NormalizedGini:0.074235 
[5]	train-NormalizedGini:0.013612 
[6]	train-NormalizedGini:0.048120 
[7]	train-NormalizedGini:0.102259 
[8]	train-NormalizedGini:0.102730 
[9]	train-NormalizedGini:0.094875 
[10]	train-NormalizedGini:0.102123 
- Fold2: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
[1]	train-NormalizedGini:0.001693 
[2]	train-NormalizedGini:0.004876 
[3]	train-NormalizedGini:0.005305 
[4]	train-NormalizedGini:0.005537 
[5]	train-NormalizedGini:-0.002458 
[6]	train-NormalizedGini:0.037353 
[7]	train-NormalizedGini:0.039941 
[8]	train-NormalizedGini:0.057317 
[9]	train-NormalizedGini:0.005332 
[10]	train-NormalizedGini:0.014446 
- Fold3: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
[1]	train-NormalizedGini:-0.039790 
[2]	train-NormalizedGini:0.062435 
[3]	train-NormalizedGini:0.067983 
[4]	train-NormalizedGini:0.066225 
[5]	train-NormalizedGini:0.057072 
[6]	train-NormalizedGini:0.048895 
[7]	train-NormalizedGini:0.038767 
[8]	train-NormalizedGini:0.036565 
[9]	train-NormalizedGini:0.021900 
[10]	train-NormalizedGini:0.010935 
eXtreme Gradient Boosting 

476753 samples
   185 predictor
     2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 317835, 317835, 317836 
Resampling results:

  NormalizedGini
  0.2243891     

Tuning parameter 'nrounds' was held constant at a value of 10
Tuning parameter 'max_depth' was held constant at a value of 5
Tuning parameter
 held constant at a value of 1
Tuning parameter 'min_child_weight' was held constant at a value of 120
Tuning parameter 'subsample' was held
 constant at a value of 0.3
   user  system elapsed 
177.817   2.663  33.292 
Class 'xgb.DMatrix' <externalptr> 
 - attr(*, ".Dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:186] "id" "ps_ind_01" "ps_ind_02_cat1" "ps_ind_02_cat2" ...
Class 'xgb.DMatrix' <externalptr> 
Class 'xgb.DMatrix' <externalptr> 
 - attr(*, ".Dimnames")=List of 2
  ..$ : NULL
  ..$ : chr [1:186] "id" "ps_ind_01" "ps_ind_02_cat1" "ps_ind_02_cat2" ...
+ Fold1: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
[1]	train-NormalizedGini:-0.006324 
[10]	train-NormalizedGini:-0.017102 
- Fold1: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold2: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
[1]	train-NormalizedGini:0.003605 
[10]	train-NormalizedGini:-0.042775 
- Fold2: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
+ Fold3: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
[1]	train-NormalizedGini:0.002399 
[10]	train-NormalizedGini:-0.023091 
- Fold3: max_depth=5, eta=0.03, min_child_weight=120, subsample=0.3, colsample_bytree=1, nrounds=10, gamma=0 
Aggregating results
Fitting final model on full training set
[1]	train-NormalizedGini:-0.036628 
[10]	train-NormalizedGini:-0.009244 
eXtreme Gradient Boosting 

476753 samples
   185 predictor
     2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Cross-Validated (3 fold) 
Summary of sample sizes: 317835, 317836, 317835 
Resampling results:

  NormalizedGini
  0.2261152     

Tuning parameter 'nrounds' was held constant at a value of 10
Tuning parameter 'max_depth' was held constant at a value of 5
Tuning parameter
 held constant at a value of 1
Tuning parameter 'min_child_weight' was held constant at a value of 120
Tuning parameter 'subsample' was held
 constant at a value of 0.3
   user  system elapsed 
173.472   1.742  30.829 
elapsed 
 792.85 
   max_depth  eta min_child_weight subsample colsample_bytree nrounds gamma
1          5 0.03              120       0.3                1     500     0
2          5 0.03              120       0.3                1    1000     0
3          5 0.03              120       0.3                1    1500     0
4          5 0.03              120       0.3                1     500     4
5          5 0.03              120       0.3                1    1000     4
6          5 0.03              120       0.3                1    1500     4
7          5 0.03              120       0.3                1     500     8
8          5 0.03              120       0.3                1    1000     8
9          5 0.03              120       0.3                1    1500     8
10         5 0.03              120       0.3                1     500    12
11         5 0.03              120       0.3                1    1000    12
12         5 0.03              120       0.3                1    1500    12
13         5 0.03              120       0.3                1     500    20
14         5 0.03              120       0.3                1    1000    20
15         5 0.03              120       0.3                1    1500    20
16         5 0.03              120       0.3                1     500    30
17         5 0.03              120       0.3                1    1000    30
18         5 0.03              120       0.3                1    1500    30
+ Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.087359 
[101]	train-NormalizedGini:0.026973 
[201]	train-NormalizedGini:0.027523 
[301]	train-NormalizedGini:0.032951 
[401]	train-NormalizedGini:0.024058 
[501]	train-NormalizedGini:0.028057 
[601]	train-NormalizedGini:0.013448 
[701]	train-NormalizedGini:0.011790 
[801]	train-NormalizedGini:0.017249 
[901]	train-NormalizedGini:0.017778 
[1001]	train-NormalizedGini:0.011063 
[1101]	train-NormalizedGini:0.011399 
[1201]	train-NormalizedGini:0.012270 
[1301]	train-NormalizedGini:0.013604 
[1401]	train-NormalizedGini:-0.000924 
[1500]	train-NormalizedGini:-0.006813 
- Fold01: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.071130 
[101]	train-NormalizedGini:0.037199 
[201]	train-NormalizedGini:0.021317 
[301]	train-NormalizedGini:0.029061 
[401]	train-NormalizedGini:0.031144 
[501]	train-NormalizedGini:0.033300 
[601]	train-NormalizedGini:0.030092 
[701]	train-NormalizedGini:0.037186 
[801]	train-NormalizedGini:0.030774 
[901]	train-NormalizedGini:0.019999 
[1001]	train-NormalizedGini:0.025975 
[1101]	train-NormalizedGini:0.020366 
[1201]	train-NormalizedGini:0.016586 
[1301]	train-NormalizedGini:0.017184 
[1401]	train-NormalizedGini:0.008630 
[1500]	train-NormalizedGini:0.006647 
- Fold01: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.071171 
[101]	train-NormalizedGini:0.028561 
[201]	train-NormalizedGini:0.022872 
[301]	train-NormalizedGini:0.029966 
[401]	train-NormalizedGini:0.030461 
[501]	train-NormalizedGini:0.033247 
[601]	train-NormalizedGini:0.017710 
[701]	train-NormalizedGini:0.013800 
[801]	train-NormalizedGini:0.015142 
[901]	train-NormalizedGini:0.005290 
[1001]	train-NormalizedGini:-0.004647 
[1101]	train-NormalizedGini:-0.005321 
[1201]	train-NormalizedGini:0.000287 
[1301]	train-NormalizedGini:0.004148 
[1401]	train-NormalizedGini:0.000130 
[1500]	train-NormalizedGini:-0.006173 
- Fold01: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.071174 
[101]	train-NormalizedGini:0.041073 
[201]	train-NormalizedGini:0.027189 
[301]	train-NormalizedGini:0.029259 
[401]	train-NormalizedGini:0.038348 
[501]	train-NormalizedGini:0.034457 
[601]	train-NormalizedGini:0.039363 
[701]	train-NormalizedGini:0.042515 
[801]	train-NormalizedGini:0.045402 
[901]	train-NormalizedGini:0.046706 
[1001]	train-NormalizedGini:0.047743 
[1101]	train-NormalizedGini:0.049131 
[1201]	train-NormalizedGini:0.044297 
[1301]	train-NormalizedGini:0.043461 
[1401]	train-NormalizedGini:0.039871 
[1500]	train-NormalizedGini:0.036815 
- Fold01: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.038397 
[201]	train-NormalizedGini:0.057547 
[301]	train-NormalizedGini:0.059825 
[401]	train-NormalizedGini:0.057194 
[501]	train-NormalizedGini:0.052728 
[601]	train-NormalizedGini:0.052859 
[701]	train-NormalizedGini:0.054260 
[801]	train-NormalizedGini:0.057108 
[901]	train-NormalizedGini:0.053260 
[1001]	train-NormalizedGini:0.053543 
[1101]	train-NormalizedGini:0.053502 
[1201]	train-NormalizedGini:0.057549 
[1301]	train-NormalizedGini:0.057404 
[1401]	train-NormalizedGini:0.058356 
[1500]	train-NormalizedGini:0.056573 
- Fold01: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold01: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002802 
[101]	train-NormalizedGini:0.082049 
[201]	train-NormalizedGini:0.083318 
[301]	train-NormalizedGini:0.079410 
[401]	train-NormalizedGini:0.082013 
[501]	train-NormalizedGini:0.083675 
[601]	train-NormalizedGini:0.082573 
[701]	train-NormalizedGini:0.082256 
[801]	train-NormalizedGini:0.081948 
[901]	train-NormalizedGini:0.081767 
[1001]	train-NormalizedGini:0.082097 
[1101]	train-NormalizedGini:0.082097 
[1201]	train-NormalizedGini:0.082446 
[1301]	train-NormalizedGini:0.083457 
[1401]	train-NormalizedGini:0.083185 
[1500]	train-NormalizedGini:0.083185 
- Fold01: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.077745 
[101]	train-NormalizedGini:0.005556 
[201]	train-NormalizedGini:0.033596 
[301]	train-NormalizedGini:0.036044 
[401]	train-NormalizedGini:0.050213 
[501]	train-NormalizedGini:0.047255 
[601]	train-NormalizedGini:0.047171 
[701]	train-NormalizedGini:0.050441 
[801]	train-NormalizedGini:0.035176 
[901]	train-NormalizedGini:0.043123 
[1001]	train-NormalizedGini:0.043449 
[1101]	train-NormalizedGini:0.054401 
[1201]	train-NormalizedGini:0.059497 
[1301]	train-NormalizedGini:0.061233 
[1401]	train-NormalizedGini:0.058667 
[1500]	train-NormalizedGini:0.063096 
- Fold02: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.052778 
[101]	train-NormalizedGini:0.011491 
[201]	train-NormalizedGini:0.026666 
[301]	train-NormalizedGini:0.035855 
[401]	train-NormalizedGini:0.037445 
[501]	train-NormalizedGini:0.027031 
[601]	train-NormalizedGini:0.019955 
[701]	train-NormalizedGini:0.028635 
[801]	train-NormalizedGini:0.028244 
[901]	train-NormalizedGini:0.020979 
[1001]	train-NormalizedGini:0.012698 
[1101]	train-NormalizedGini:0.017052 
[1201]	train-NormalizedGini:0.024466 
[1301]	train-NormalizedGini:0.015787 
[1401]	train-NormalizedGini:0.010746 
[1500]	train-NormalizedGini:0.003688 
- Fold02: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.010075 
[201]	train-NormalizedGini:0.027801 
[301]	train-NormalizedGini:0.031749 
[401]	train-NormalizedGini:0.029333 
[501]	train-NormalizedGini:0.017672 
[601]	train-NormalizedGini:0.017632 
[701]	train-NormalizedGini:0.010981 
[801]	train-NormalizedGini:0.009228 
[901]	train-NormalizedGini:0.016848 
[1001]	train-NormalizedGini:0.022681 
[1101]	train-NormalizedGini:0.018382 
[1201]	train-NormalizedGini:0.016595 
[1301]	train-NormalizedGini:0.010896 
[1401]	train-NormalizedGini:0.011642 
[1500]	train-NormalizedGini:0.007096 
- Fold02: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.047865 
[201]	train-NormalizedGini:0.047298 
[301]	train-NormalizedGini:0.051116 
[401]	train-NormalizedGini:0.051724 
[501]	train-NormalizedGini:0.048319 
[601]	train-NormalizedGini:0.047512 
[701]	train-NormalizedGini:0.050312 
[801]	train-NormalizedGini:0.052321 
[901]	train-NormalizedGini:0.056966 
[1001]	train-NormalizedGini:0.056091 
[1101]	train-NormalizedGini:0.050638 
[1201]	train-NormalizedGini:0.051017 
[1301]	train-NormalizedGini:0.053873 
[1401]	train-NormalizedGini:0.055136 
[1500]	train-NormalizedGini:0.058484 
- Fold02: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.050371 
[201]	train-NormalizedGini:0.055194 
[301]	train-NormalizedGini:0.050827 
[401]	train-NormalizedGini:0.051649 
[501]	train-NormalizedGini:0.053124 
[601]	train-NormalizedGini:0.054140 
[701]	train-NormalizedGini:0.055913 
[801]	train-NormalizedGini:0.054630 
[901]	train-NormalizedGini:0.055620 
[1001]	train-NormalizedGini:0.054092 
[1101]	train-NormalizedGini:0.054463 
[1201]	train-NormalizedGini:0.055206 
[1301]	train-NormalizedGini:0.056119 
[1401]	train-NormalizedGini:0.055773 
[1500]	train-NormalizedGini:0.056178 
- Fold02: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold02: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.042630 
[201]	train-NormalizedGini:0.054138 
[301]	train-NormalizedGini:0.057489 
[401]	train-NormalizedGini:0.063565 
[501]	train-NormalizedGini:0.066427 
[601]	train-NormalizedGini:0.067162 
[701]	train-NormalizedGini:0.067594 
[801]	train-NormalizedGini:0.067823 
[901]	train-NormalizedGini:0.068256 
[1001]	train-NormalizedGini:0.068182 
[1101]	train-NormalizedGini:0.069630 
[1201]	train-NormalizedGini:0.069631 
[1301]	train-NormalizedGini:0.069630 
[1401]	train-NormalizedGini:0.068868 
[1500]	train-NormalizedGini:0.068859 
- Fold02: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.077386 
[101]	train-NormalizedGini:0.035307 
[201]	train-NormalizedGini:0.049545 
[301]	train-NormalizedGini:0.053124 
[401]	train-NormalizedGini:0.056399 
[501]	train-NormalizedGini:0.043998 
[601]	train-NormalizedGini:0.043355 
[701]	train-NormalizedGini:0.040076 
[801]	train-NormalizedGini:0.041903 
[901]	train-NormalizedGini:0.034031 
[1001]	train-NormalizedGini:0.032684 
[1101]	train-NormalizedGini:0.027939 
[1201]	train-NormalizedGini:0.021299 
[1301]	train-NormalizedGini:0.025554 
[1401]	train-NormalizedGini:0.025883 
[1500]	train-NormalizedGini:0.019398 
- Fold03: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.062515 
[101]	train-NormalizedGini:0.017128 
[201]	train-NormalizedGini:0.038745 
[301]	train-NormalizedGini:0.043595 
[401]	train-NormalizedGini:0.036538 
[501]	train-NormalizedGini:0.018545 
[601]	train-NormalizedGini:0.014918 
[701]	train-NormalizedGini:0.010510 
[801]	train-NormalizedGini:0.011237 
[901]	train-NormalizedGini:0.016207 
[1001]	train-NormalizedGini:0.017155 
[1101]	train-NormalizedGini:0.023860 
[1201]	train-NormalizedGini:0.036891 
[1301]	train-NormalizedGini:0.029737 
[1401]	train-NormalizedGini:0.024364 
[1500]	train-NormalizedGini:0.017618 
- Fold03: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.071130 
[101]	train-NormalizedGini:0.026189 
[201]	train-NormalizedGini:0.038053 
[301]	train-NormalizedGini:0.048817 
[401]	train-NormalizedGini:0.041846 
[501]	train-NormalizedGini:0.039168 
[601]	train-NormalizedGini:0.039284 
[701]	train-NormalizedGini:0.040594 
[801]	train-NormalizedGini:0.034597 
[901]	train-NormalizedGini:0.029570 
[1001]	train-NormalizedGini:0.030442 
[1101]	train-NormalizedGini:0.027867 
[1201]	train-NormalizedGini:0.022425 
[1301]	train-NormalizedGini:0.027051 
[1401]	train-NormalizedGini:0.017136 
[1500]	train-NormalizedGini:0.019546 
- Fold03: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.057706 
[201]	train-NormalizedGini:0.070960 
[301]	train-NormalizedGini:0.076439 
[401]	train-NormalizedGini:0.072494 
[501]	train-NormalizedGini:0.077081 
[601]	train-NormalizedGini:0.079194 
[701]	train-NormalizedGini:0.082135 
[801]	train-NormalizedGini:0.081448 
[901]	train-NormalizedGini:0.079317 
[1001]	train-NormalizedGini:0.079291 
[1101]	train-NormalizedGini:0.079124 
[1201]	train-NormalizedGini:0.076607 
[1301]	train-NormalizedGini:0.072272 
[1401]	train-NormalizedGini:0.071796 
[1500]	train-NormalizedGini:0.068289 
- Fold03: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.050965 
[201]	train-NormalizedGini:0.070995 
[301]	train-NormalizedGini:0.076753 
[401]	train-NormalizedGini:0.076635 
[501]	train-NormalizedGini:0.074613 
[601]	train-NormalizedGini:0.074019 
[701]	train-NormalizedGini:0.073083 
[801]	train-NormalizedGini:0.073028 
[901]	train-NormalizedGini:0.074189 
[1001]	train-NormalizedGini:0.072783 
[1101]	train-NormalizedGini:0.072783 
[1201]	train-NormalizedGini:0.072010 
[1301]	train-NormalizedGini:0.072487 
[1401]	train-NormalizedGini:0.070833 
[1500]	train-NormalizedGini:0.070833 
- Fold03: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold03: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002404 
[101]	train-NormalizedGini:0.052694 
[201]	train-NormalizedGini:0.061848 
[301]	train-NormalizedGini:0.062471 
[401]	train-NormalizedGini:0.066817 
[501]	train-NormalizedGini:0.067152 
[601]	train-NormalizedGini:0.066147 
[701]	train-NormalizedGini:0.065905 
[801]	train-NormalizedGini:0.065827 
[901]	train-NormalizedGini:0.066707 
[1001]	train-NormalizedGini:0.068049 
[1101]	train-NormalizedGini:0.067686 
[1201]	train-NormalizedGini:0.067921 
[1301]	train-NormalizedGini:0.068530 
[1401]	train-NormalizedGini:0.070507 
[1500]	train-NormalizedGini:0.070507 
- Fold03: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.069155 
[101]	train-NormalizedGini:-0.014169 
[201]	train-NormalizedGini:0.008563 
[301]	train-NormalizedGini:0.012431 
[401]	train-NormalizedGini:-0.000671 
[501]	train-NormalizedGini:0.000802 
[601]	train-NormalizedGini:0.007115 
[701]	train-NormalizedGini:-0.008291 
[801]	train-NormalizedGini:0.004318 
[901]	train-NormalizedGini:0.010977 
[1001]	train-NormalizedGini:0.009863 
[1101]	train-NormalizedGini:0.007036 
[1201]	train-NormalizedGini:0.002259 
[1301]	train-NormalizedGini:-0.001355 
[1401]	train-NormalizedGini:0.006039 
[1500]	train-NormalizedGini:0.006993 
- Fold04: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.052778 
[101]	train-NormalizedGini:0.026016 
[201]	train-NormalizedGini:0.010483 
[301]	train-NormalizedGini:0.013365 
[401]	train-NormalizedGini:0.000284 
[501]	train-NormalizedGini:0.003959 
[601]	train-NormalizedGini:0.000217 
[701]	train-NormalizedGini:-0.001772 
[801]	train-NormalizedGini:0.004321 
[901]	train-NormalizedGini:-0.002263 
[1001]	train-NormalizedGini:-0.005288 
[1101]	train-NormalizedGini:-0.007396 
[1201]	train-NormalizedGini:0.000546 
[1301]	train-NormalizedGini:-0.007106 
[1401]	train-NormalizedGini:-0.017502 
[1500]	train-NormalizedGini:-0.014906 
- Fold04: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.071174 
[101]	train-NormalizedGini:0.019572 
[201]	train-NormalizedGini:0.023371 
[301]	train-NormalizedGini:0.033248 
[401]	train-NormalizedGini:0.040678 
[501]	train-NormalizedGini:0.033072 
[601]	train-NormalizedGini:0.049563 
[701]	train-NormalizedGini:0.035537 
[801]	train-NormalizedGini:0.028848 
[901]	train-NormalizedGini:0.023007 
[1001]	train-NormalizedGini:0.023845 
[1101]	train-NormalizedGini:0.029145 
[1201]	train-NormalizedGini:0.035956 
[1301]	train-NormalizedGini:0.036589 
[1401]	train-NormalizedGini:0.028755 
[1500]	train-NormalizedGini:0.019989 
- Fold04: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002404 
[101]	train-NormalizedGini:0.033407 
[201]	train-NormalizedGini:0.033690 
[301]	train-NormalizedGini:0.033212 
[401]	train-NormalizedGini:0.034687 
[501]	train-NormalizedGini:0.033059 
[601]	train-NormalizedGini:0.032827 
[701]	train-NormalizedGini:0.030958 
[801]	train-NormalizedGini:0.034620 
[901]	train-NormalizedGini:0.042471 
[1001]	train-NormalizedGini:0.039949 
[1101]	train-NormalizedGini:0.036188 
[1201]	train-NormalizedGini:0.036832 
[1301]	train-NormalizedGini:0.034230 
[1401]	train-NormalizedGini:0.032305 
[1500]	train-NormalizedGini:0.033327 
- Fold04: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.046440 
[201]	train-NormalizedGini:0.044727 
[301]	train-NormalizedGini:0.054298 
[401]	train-NormalizedGini:0.058749 
[501]	train-NormalizedGini:0.058391 
[601]	train-NormalizedGini:0.058801 
[701]	train-NormalizedGini:0.060153 
[801]	train-NormalizedGini:0.061303 
[901]	train-NormalizedGini:0.060808 
[1001]	train-NormalizedGini:0.060383 
[1101]	train-NormalizedGini:0.059729 
[1201]	train-NormalizedGini:0.058132 
[1301]	train-NormalizedGini:0.058680 
[1401]	train-NormalizedGini:0.059977 
[1500]	train-NormalizedGini:0.059977 
- Fold04: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold04: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.022327 
[201]	train-NormalizedGini:0.055327 
[301]	train-NormalizedGini:0.060805 
[401]	train-NormalizedGini:0.063624 
[501]	train-NormalizedGini:0.063292 
[601]	train-NormalizedGini:0.061672 
[701]	train-NormalizedGini:0.061665 
[801]	train-NormalizedGini:0.061535 
[901]	train-NormalizedGini:0.061743 
[1001]	train-NormalizedGini:0.062845 
[1101]	train-NormalizedGini:0.062834 
[1201]	train-NormalizedGini:0.061242 
[1301]	train-NormalizedGini:0.062942 
[1401]	train-NormalizedGini:0.059763 
[1500]	train-NormalizedGini:0.054250 
- Fold04: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.052443 
[101]	train-NormalizedGini:-0.020602 
[201]	train-NormalizedGini:-0.016890 
[301]	train-NormalizedGini:-0.014254 
[401]	train-NormalizedGini:-0.024122 
[501]	train-NormalizedGini:-0.021805 
[601]	train-NormalizedGini:-0.023841 
[701]	train-NormalizedGini:-0.018666 
[801]	train-NormalizedGini:-0.025596 
[901]	train-NormalizedGini:-0.026593 
[1001]	train-NormalizedGini:-0.023227 
[1101]	train-NormalizedGini:-0.020978 
[1201]	train-NormalizedGini:-0.030422 
[1301]	train-NormalizedGini:-0.029532 
[1401]	train-NormalizedGini:-0.032590 
[1500]	train-NormalizedGini:-0.023402 
- Fold05: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.063407 
[101]	train-NormalizedGini:0.036718 
[201]	train-NormalizedGini:0.035033 
[301]	train-NormalizedGini:0.037460 
[401]	train-NormalizedGini:0.025866 
[501]	train-NormalizedGini:0.027067 
[601]	train-NormalizedGini:0.031253 
[701]	train-NormalizedGini:0.021923 
[801]	train-NormalizedGini:0.021732 
[901]	train-NormalizedGini:0.024282 
[1001]	train-NormalizedGini:0.031506 
[1101]	train-NormalizedGini:0.028430 
[1201]	train-NormalizedGini:0.032222 
[1301]	train-NormalizedGini:0.038627 
[1401]	train-NormalizedGini:0.034693 
[1500]	train-NormalizedGini:0.032164 
- Fold05: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.071174 
[101]	train-NormalizedGini:0.040738 
[201]	train-NormalizedGini:0.033683 
[301]	train-NormalizedGini:0.029967 
[401]	train-NormalizedGini:0.046067 
[501]	train-NormalizedGini:0.042749 
[601]	train-NormalizedGini:0.040549 
[701]	train-NormalizedGini:0.046125 
[801]	train-NormalizedGini:0.049887 
[901]	train-NormalizedGini:0.043337 
[1001]	train-NormalizedGini:0.040914 
[1101]	train-NormalizedGini:0.042050 
[1201]	train-NormalizedGini:0.038395 
[1301]	train-NormalizedGini:0.040885 
[1401]	train-NormalizedGini:0.048948 
[1500]	train-NormalizedGini:0.048651 
- Fold05: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.051511 
[201]	train-NormalizedGini:0.061858 
[301]	train-NormalizedGini:0.073132 
[401]	train-NormalizedGini:0.072240 
[501]	train-NormalizedGini:0.073729 
[601]	train-NormalizedGini:0.078980 
[701]	train-NormalizedGini:0.078010 
[801]	train-NormalizedGini:0.077925 
[901]	train-NormalizedGini:0.081242 
[1001]	train-NormalizedGini:0.077622 
[1101]	train-NormalizedGini:0.077596 
[1201]	train-NormalizedGini:0.073557 
[1301]	train-NormalizedGini:0.069450 
[1401]	train-NormalizedGini:0.070011 
[1500]	train-NormalizedGini:0.068727 
- Fold05: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.068418 
[201]	train-NormalizedGini:0.072705 
[301]	train-NormalizedGini:0.077365 
[401]	train-NormalizedGini:0.077718 
[501]	train-NormalizedGini:0.076569 
[601]	train-NormalizedGini:0.075365 
[701]	train-NormalizedGini:0.077931 
[801]	train-NormalizedGini:0.077240 
[901]	train-NormalizedGini:0.077563 
[1001]	train-NormalizedGini:0.077423 
[1101]	train-NormalizedGini:0.074599 
[1201]	train-NormalizedGini:0.074789 
[1301]	train-NormalizedGini:0.075184 
[1401]	train-NormalizedGini:0.075225 
[1500]	train-NormalizedGini:0.075368 
- Fold05: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold05: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.068045 
[201]	train-NormalizedGini:0.070255 
[301]	train-NormalizedGini:0.070743 
[401]	train-NormalizedGini:0.071946 
[501]	train-NormalizedGini:0.074312 
[601]	train-NormalizedGini:0.075700 
[701]	train-NormalizedGini:0.075455 
[801]	train-NormalizedGini:0.075673 
[901]	train-NormalizedGini:0.076164 
[1001]	train-NormalizedGini:0.075235 
[1101]	train-NormalizedGini:0.075438 
[1201]	train-NormalizedGini:0.076040 
[1301]	train-NormalizedGini:0.074262 
[1401]	train-NormalizedGini:0.074258 
[1500]	train-NormalizedGini:0.074881 
- Fold05: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.067317 
[101]	train-NormalizedGini:0.018534 
[201]	train-NormalizedGini:0.047806 
[301]	train-NormalizedGini:0.023851 
[401]	train-NormalizedGini:0.034419 
[501]	train-NormalizedGini:0.036873 
[601]	train-NormalizedGini:0.045584 
[701]	train-NormalizedGini:0.037400 
[801]	train-NormalizedGini:0.044210 
[901]	train-NormalizedGini:0.031715 
[1001]	train-NormalizedGini:0.025562 
[1101]	train-NormalizedGini:0.027387 
[1201]	train-NormalizedGini:0.034447 
[1301]	train-NormalizedGini:0.036498 
[1401]	train-NormalizedGini:0.047431 
[1500]	train-NormalizedGini:0.044648 
- Fold06: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.039191 
[101]	train-NormalizedGini:0.003173 
[201]	train-NormalizedGini:0.008663 
[301]	train-NormalizedGini:0.006383 
[401]	train-NormalizedGini:0.003899 
[501]	train-NormalizedGini:-0.015027 
[601]	train-NormalizedGini:-0.016750 
[701]	train-NormalizedGini:-0.008854 
[801]	train-NormalizedGini:-0.015907 
[901]	train-NormalizedGini:-0.021029 
[1001]	train-NormalizedGini:-0.026177 
[1101]	train-NormalizedGini:-0.027937 
[1201]	train-NormalizedGini:-0.021469 
[1301]	train-NormalizedGini:-0.019424 
[1401]	train-NormalizedGini:-0.018101 
[1500]	train-NormalizedGini:0.003048 
- Fold06: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.071174 
[101]	train-NormalizedGini:0.018176 
[201]	train-NormalizedGini:0.032231 
[301]	train-NormalizedGini:0.029889 
[401]	train-NormalizedGini:0.031136 
[501]	train-NormalizedGini:0.038255 
[601]	train-NormalizedGini:0.040991 
[701]	train-NormalizedGini:0.037157 
[801]	train-NormalizedGini:0.042833 
[901]	train-NormalizedGini:0.033323 
[1001]	train-NormalizedGini:0.034380 
[1101]	train-NormalizedGini:0.039140 
[1201]	train-NormalizedGini:0.043997 
[1301]	train-NormalizedGini:0.033837 
[1401]	train-NormalizedGini:0.038872 
[1500]	train-NormalizedGini:0.032670 
- Fold06: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.038686 
[201]	train-NormalizedGini:0.034617 
[301]	train-NormalizedGini:0.030335 
[401]	train-NormalizedGini:0.030981 
[501]	train-NormalizedGini:0.036262 
[601]	train-NormalizedGini:0.031295 
[701]	train-NormalizedGini:0.036900 
[801]	train-NormalizedGini:0.035234 
[901]	train-NormalizedGini:0.034170 
[1001]	train-NormalizedGini:0.033681 
[1101]	train-NormalizedGini:0.032944 
[1201]	train-NormalizedGini:0.035722 
[1301]	train-NormalizedGini:0.036153 
[1401]	train-NormalizedGini:0.032556 
[1500]	train-NormalizedGini:0.031296 
- Fold06: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.018668 
[201]	train-NormalizedGini:0.046588 
[301]	train-NormalizedGini:0.048822 
[401]	train-NormalizedGini:0.050320 
[501]	train-NormalizedGini:0.050442 
[601]	train-NormalizedGini:0.051937 
[701]	train-NormalizedGini:0.053190 
[801]	train-NormalizedGini:0.051501 
[901]	train-NormalizedGini:0.053666 
[1001]	train-NormalizedGini:0.053919 
[1101]	train-NormalizedGini:0.053791 
[1201]	train-NormalizedGini:0.054683 
[1301]	train-NormalizedGini:0.053511 
[1401]	train-NormalizedGini:0.054353 
[1500]	train-NormalizedGini:0.053976 
- Fold06: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold06: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002404 
[101]	train-NormalizedGini:0.073085 
[201]	train-NormalizedGini:0.067596 
[301]	train-NormalizedGini:0.073168 
[401]	train-NormalizedGini:0.072676 
[501]	train-NormalizedGini:0.070795 
[601]	train-NormalizedGini:0.071804 
[701]	train-NormalizedGini:0.070808 
[801]	train-NormalizedGini:0.071069 
[901]	train-NormalizedGini:0.069617 
[1001]	train-NormalizedGini:0.068730 
[1101]	train-NormalizedGini:0.068483 
[1201]	train-NormalizedGini:0.068483 
[1301]	train-NormalizedGini:0.067430 
[1401]	train-NormalizedGini:0.066430 
[1500]	train-NormalizedGini:0.065648 
- Fold06: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.080313 
[101]	train-NormalizedGini:-0.014724 
[201]	train-NormalizedGini:-0.013135 
[301]	train-NormalizedGini:-0.015285 
[401]	train-NormalizedGini:-0.018575 
[501]	train-NormalizedGini:-0.001059 
[601]	train-NormalizedGini:0.007825 
[701]	train-NormalizedGini:-0.005221 
[801]	train-NormalizedGini:-0.006569 
[901]	train-NormalizedGini:-0.012740 
[1001]	train-NormalizedGini:-0.014389 
[1101]	train-NormalizedGini:-0.023180 
[1201]	train-NormalizedGini:-0.009524 
[1301]	train-NormalizedGini:-0.013931 
[1401]	train-NormalizedGini:-0.013199 
[1500]	train-NormalizedGini:-0.009937 
- Fold07: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.039718 
[101]	train-NormalizedGini:0.023759 
[201]	train-NormalizedGini:0.032597 
[301]	train-NormalizedGini:0.019628 
[401]	train-NormalizedGini:0.020910 
[501]	train-NormalizedGini:0.027447 
[601]	train-NormalizedGini:0.021023 
[701]	train-NormalizedGini:0.019426 
[801]	train-NormalizedGini:0.009220 
[901]	train-NormalizedGini:0.014701 
[1001]	train-NormalizedGini:0.010091 
[1101]	train-NormalizedGini:0.009551 
[1201]	train-NormalizedGini:0.004713 
[1301]	train-NormalizedGini:0.014948 
[1401]	train-NormalizedGini:0.012923 
[1500]	train-NormalizedGini:0.013039 
- Fold07: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.071174 
[101]	train-NormalizedGini:0.049940 
[201]	train-NormalizedGini:0.033381 
[301]	train-NormalizedGini:0.020346 
[401]	train-NormalizedGini:0.010792 
[501]	train-NormalizedGini:0.021403 
[601]	train-NormalizedGini:0.009816 
[701]	train-NormalizedGini:0.013957 
[801]	train-NormalizedGini:0.015503 
[901]	train-NormalizedGini:0.008999 
[1001]	train-NormalizedGini:0.013335 
[1101]	train-NormalizedGini:0.014580 
[1201]	train-NormalizedGini:0.013431 
[1301]	train-NormalizedGini:0.009459 
[1401]	train-NormalizedGini:0.008171 
[1500]	train-NormalizedGini:0.009421 
- Fold07: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.043605 
[201]	train-NormalizedGini:0.072099 
[301]	train-NormalizedGini:0.077203 
[401]	train-NormalizedGini:0.063614 
[501]	train-NormalizedGini:0.063892 
[601]	train-NormalizedGini:0.058270 
[701]	train-NormalizedGini:0.061281 
[801]	train-NormalizedGini:0.058456 
[901]	train-NormalizedGini:0.057432 
[1001]	train-NormalizedGini:0.060006 
[1101]	train-NormalizedGini:0.063107 
[1201]	train-NormalizedGini:0.064750 
[1301]	train-NormalizedGini:0.067015 
[1401]	train-NormalizedGini:0.065114 
[1500]	train-NormalizedGini:0.062670 
- Fold07: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.047777 
[201]	train-NormalizedGini:0.058125 
[301]	train-NormalizedGini:0.054113 
[401]	train-NormalizedGini:0.052519 
[501]	train-NormalizedGini:0.051777 
[601]	train-NormalizedGini:0.053040 
[701]	train-NormalizedGini:0.052989 
[801]	train-NormalizedGini:0.052526 
[901]	train-NormalizedGini:0.051705 
[1001]	train-NormalizedGini:0.054335 
[1101]	train-NormalizedGini:0.054454 
[1201]	train-NormalizedGini:0.054326 
[1301]	train-NormalizedGini:0.055545 
[1401]	train-NormalizedGini:0.055240 
[1500]	train-NormalizedGini:0.057629 
- Fold07: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold07: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.047252 
[201]	train-NormalizedGini:0.065082 
[301]	train-NormalizedGini:0.069461 
[401]	train-NormalizedGini:0.069922 
[501]	train-NormalizedGini:0.065899 
[601]	train-NormalizedGini:0.064353 
[701]	train-NormalizedGini:0.065702 
[801]	train-NormalizedGini:0.065024 
[901]	train-NormalizedGini:0.065938 
[1001]	train-NormalizedGini:0.065939 
[1101]	train-NormalizedGini:0.068692 
[1201]	train-NormalizedGini:0.070379 
[1301]	train-NormalizedGini:0.067684 
[1401]	train-NormalizedGini:0.067512 
[1500]	train-NormalizedGini:0.068421 
- Fold07: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.050744 
[101]	train-NormalizedGini:-0.011380 
[201]	train-NormalizedGini:-0.010548 
[301]	train-NormalizedGini:-0.006891 
[401]	train-NormalizedGini:-0.026899 
[501]	train-NormalizedGini:-0.018653 
[601]	train-NormalizedGini:-0.025225 
[701]	train-NormalizedGini:-0.027830 
[801]	train-NormalizedGini:-0.024697 
[901]	train-NormalizedGini:-0.023562 
[1001]	train-NormalizedGini:-0.030880 
[1101]	train-NormalizedGini:-0.039773 
[1201]	train-NormalizedGini:-0.042590 
[1301]	train-NormalizedGini:-0.044058 
[1401]	train-NormalizedGini:-0.039572 
[1500]	train-NormalizedGini:-0.030723 
- Fold08: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.071105 
[101]	train-NormalizedGini:0.020550 
[201]	train-NormalizedGini:0.030527 
[301]	train-NormalizedGini:0.027757 
[401]	train-NormalizedGini:0.016228 
[501]	train-NormalizedGini:0.002187 
[601]	train-NormalizedGini:0.001222 
[701]	train-NormalizedGini:-0.002484 
[801]	train-NormalizedGini:0.000540 
[901]	train-NormalizedGini:0.011076 
[1001]	train-NormalizedGini:0.009757 
[1101]	train-NormalizedGini:0.013373 
[1201]	train-NormalizedGini:0.014747 
[1301]	train-NormalizedGini:0.027465 
[1401]	train-NormalizedGini:0.023141 
[1500]	train-NormalizedGini:0.024185 
- Fold08: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.045528 
[101]	train-NormalizedGini:0.013652 
[201]	train-NormalizedGini:0.045091 
[301]	train-NormalizedGini:0.048691 
[401]	train-NormalizedGini:0.035148 
[501]	train-NormalizedGini:0.032000 
[601]	train-NormalizedGini:0.038625 
[701]	train-NormalizedGini:0.032385 
[801]	train-NormalizedGini:0.029947 
[901]	train-NormalizedGini:0.025181 
[1001]	train-NormalizedGini:0.024878 
[1101]	train-NormalizedGini:0.023565 
[1201]	train-NormalizedGini:0.018034 
[1301]	train-NormalizedGini:0.017470 
[1401]	train-NormalizedGini:0.018949 
[1500]	train-NormalizedGini:0.021168 
- Fold08: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.071174 
[101]	train-NormalizedGini:0.045755 
[201]	train-NormalizedGini:0.049151 
[301]	train-NormalizedGini:0.046439 
[401]	train-NormalizedGini:0.050537 
[501]	train-NormalizedGini:0.052038 
[601]	train-NormalizedGini:0.049845 
[701]	train-NormalizedGini:0.051961 
[801]	train-NormalizedGini:0.048673 
[901]	train-NormalizedGini:0.050440 
[1001]	train-NormalizedGini:0.059073 
[1101]	train-NormalizedGini:0.059304 
[1201]	train-NormalizedGini:0.061434 
[1301]	train-NormalizedGini:0.061313 
[1401]	train-NormalizedGini:0.055661 
[1500]	train-NormalizedGini:0.057004 
- Fold08: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.059248 
[201]	train-NormalizedGini:0.049753 
[301]	train-NormalizedGini:0.057067 
[401]	train-NormalizedGini:0.060765 
[501]	train-NormalizedGini:0.061595 
[601]	train-NormalizedGini:0.062247 
[701]	train-NormalizedGini:0.060811 
[801]	train-NormalizedGini:0.057085 
[901]	train-NormalizedGini:0.057692 
[1001]	train-NormalizedGini:0.058072 
[1101]	train-NormalizedGini:0.056999 
[1201]	train-NormalizedGini:0.056424 
[1301]	train-NormalizedGini:0.053147 
[1401]	train-NormalizedGini:0.052972 
[1500]	train-NormalizedGini:0.052972 
- Fold08: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold08: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.056247 
[201]	train-NormalizedGini:0.081565 
[301]	train-NormalizedGini:0.082951 
[401]	train-NormalizedGini:0.083028 
[501]	train-NormalizedGini:0.081975 
[601]	train-NormalizedGini:0.082206 
[701]	train-NormalizedGini:0.085329 
[801]	train-NormalizedGini:0.085240 
[901]	train-NormalizedGini:0.084950 
[1001]	train-NormalizedGini:0.084256 
[1101]	train-NormalizedGini:0.084631 
[1201]	train-NormalizedGini:0.084870 
[1301]	train-NormalizedGini:0.085775 
[1401]	train-NormalizedGini:0.084650 
[1500]	train-NormalizedGini:0.085991 
- Fold08: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.052830 
[101]	train-NormalizedGini:0.012697 
[201]	train-NormalizedGini:-0.001086 
[301]	train-NormalizedGini:-0.015234 
[401]	train-NormalizedGini:-0.030549 
[501]	train-NormalizedGini:-0.019840 
[601]	train-NormalizedGini:-0.018021 
[701]	train-NormalizedGini:-0.020183 
[801]	train-NormalizedGini:-0.027204 
[901]	train-NormalizedGini:-0.026070 
[1001]	train-NormalizedGini:-0.026098 
[1101]	train-NormalizedGini:-0.023659 
[1201]	train-NormalizedGini:-0.018671 
[1301]	train-NormalizedGini:-0.017875 
[1401]	train-NormalizedGini:-0.019066 
[1500]	train-NormalizedGini:-0.014231 
- Fold09: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.052778 
[101]	train-NormalizedGini:0.051579 
[201]	train-NormalizedGini:0.045319 
[301]	train-NormalizedGini:0.065689 
[401]	train-NormalizedGini:0.057576 
[501]	train-NormalizedGini:0.063060 
[601]	train-NormalizedGini:0.081635 
[701]	train-NormalizedGini:0.076123 
[801]	train-NormalizedGini:0.074805 
[901]	train-NormalizedGini:0.081711 
[1001]	train-NormalizedGini:0.084000 
[1101]	train-NormalizedGini:0.072415 
[1201]	train-NormalizedGini:0.079533 
[1301]	train-NormalizedGini:0.087975 
[1401]	train-NormalizedGini:0.090407 
[1500]	train-NormalizedGini:0.095601 
- Fold09: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:-0.039475 
[101]	train-NormalizedGini:0.048102 
[201]	train-NormalizedGini:0.059901 
[301]	train-NormalizedGini:0.050342 
[401]	train-NormalizedGini:0.048362 
[501]	train-NormalizedGini:0.049449 
[601]	train-NormalizedGini:0.044013 
[701]	train-NormalizedGini:0.031518 
[801]	train-NormalizedGini:0.033159 
[901]	train-NormalizedGini:0.031124 
[1001]	train-NormalizedGini:0.032470 
[1101]	train-NormalizedGini:0.041317 
[1201]	train-NormalizedGini:0.043503 
[1301]	train-NormalizedGini:0.040126 
[1401]	train-NormalizedGini:0.043246 
[1500]	train-NormalizedGini:0.046343 
- Fold09: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.036611 
[201]	train-NormalizedGini:0.041248 
[301]	train-NormalizedGini:0.059542 
[401]	train-NormalizedGini:0.062122 
[501]	train-NormalizedGini:0.059441 
[601]	train-NormalizedGini:0.060552 
[701]	train-NormalizedGini:0.058490 
[801]	train-NormalizedGini:0.056774 
[901]	train-NormalizedGini:0.060225 
[1001]	train-NormalizedGini:0.060343 
[1101]	train-NormalizedGini:0.058531 
[1201]	train-NormalizedGini:0.058850 
[1301]	train-NormalizedGini:0.065209 
[1401]	train-NormalizedGini:0.057545 
[1500]	train-NormalizedGini:0.057631 
- Fold09: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.047969 
[201]	train-NormalizedGini:0.063725 
[301]	train-NormalizedGini:0.067045 
[401]	train-NormalizedGini:0.069647 
[501]	train-NormalizedGini:0.067877 
[601]	train-NormalizedGini:0.068039 
[701]	train-NormalizedGini:0.067185 
[801]	train-NormalizedGini:0.067550 
[901]	train-NormalizedGini:0.067650 
[1001]	train-NormalizedGini:0.068821 
[1101]	train-NormalizedGini:0.068552 
[1201]	train-NormalizedGini:0.068553 
[1301]	train-NormalizedGini:0.068518 
[1401]	train-NormalizedGini:0.070602 
[1500]	train-NormalizedGini:0.069925 
- Fold09: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold09: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002802 
[101]	train-NormalizedGini:0.047919 
[201]	train-NormalizedGini:0.067542 
[301]	train-NormalizedGini:0.066978 
[401]	train-NormalizedGini:0.068024 
[501]	train-NormalizedGini:0.066185 
[601]	train-NormalizedGini:0.064356 
[701]	train-NormalizedGini:0.064456 
[801]	train-NormalizedGini:0.063791 
[901]	train-NormalizedGini:0.063743 
[1001]	train-NormalizedGini:0.063700 
[1101]	train-NormalizedGini:0.063487 
[1201]	train-NormalizedGini:0.063733 
[1301]	train-NormalizedGini:0.062838 
[1401]	train-NormalizedGini:0.063788 
[1500]	train-NormalizedGini:0.063786 
- Fold09: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.055902 
[101]	train-NormalizedGini:0.034037 
[201]	train-NormalizedGini:0.051644 
[301]	train-NormalizedGini:0.063180 
[401]	train-NormalizedGini:0.065204 
[501]	train-NormalizedGini:0.058898 
[601]	train-NormalizedGini:0.057783 
[701]	train-NormalizedGini:0.051556 
[801]	train-NormalizedGini:0.051362 
[901]	train-NormalizedGini:0.059576 
[1001]	train-NormalizedGini:0.068182 
[1101]	train-NormalizedGini:0.068130 
[1201]	train-NormalizedGini:0.068320 
[1301]	train-NormalizedGini:0.069374 
[1401]	train-NormalizedGini:0.080998 
[1500]	train-NormalizedGini:0.073312 
- Fold10: eta=0.03, max_depth=5, gamma= 0, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.073753 
[101]	train-NormalizedGini:-0.002938 
[201]	train-NormalizedGini:0.029659 
[301]	train-NormalizedGini:0.037983 
[401]	train-NormalizedGini:0.034505 
[501]	train-NormalizedGini:0.030232 
[601]	train-NormalizedGini:0.035959 
[701]	train-NormalizedGini:0.027867 
[801]	train-NormalizedGini:0.016890 
[901]	train-NormalizedGini:0.021301 
[1001]	train-NormalizedGini:0.023783 
[1101]	train-NormalizedGini:0.022844 
[1201]	train-NormalizedGini:0.020414 
[1301]	train-NormalizedGini:0.016471 
[1401]	train-NormalizedGini:0.015158 
[1500]	train-NormalizedGini:0.003498 
- Fold10: eta=0.03, max_depth=5, gamma= 4, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.009390 
[201]	train-NormalizedGini:0.019054 
[301]	train-NormalizedGini:0.034621 
[401]	train-NormalizedGini:0.050721 
[501]	train-NormalizedGini:0.055097 
[601]	train-NormalizedGini:0.053975 
[701]	train-NormalizedGini:0.053712 
[801]	train-NormalizedGini:0.046585 
[901]	train-NormalizedGini:0.051215 
[1001]	train-NormalizedGini:0.054139 
[1101]	train-NormalizedGini:0.054112 
[1201]	train-NormalizedGini:0.059235 
[1301]	train-NormalizedGini:0.057718 
[1401]	train-NormalizedGini:0.053466 
[1500]	train-NormalizedGini:0.055523 
- Fold10: eta=0.03, max_depth=5, gamma= 8, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002404 
[101]	train-NormalizedGini:0.039422 
[201]	train-NormalizedGini:0.049121 
[301]	train-NormalizedGini:0.051964 
[401]	train-NormalizedGini:0.057095 
[501]	train-NormalizedGini:0.062218 
[601]	train-NormalizedGini:0.060007 
[701]	train-NormalizedGini:0.062396 
[801]	train-NormalizedGini:0.062796 
[901]	train-NormalizedGini:0.062963 
[1001]	train-NormalizedGini:0.064495 
[1101]	train-NormalizedGini:0.067570 
[1201]	train-NormalizedGini:0.067396 
[1301]	train-NormalizedGini:0.065319 
[1401]	train-NormalizedGini:0.066582 
[1500]	train-NormalizedGini:0.064690 
- Fold10: eta=0.03, max_depth=5, gamma=12, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002720 
[101]	train-NormalizedGini:0.059669 
[201]	train-NormalizedGini:0.049814 
[301]	train-NormalizedGini:0.052918 
[401]	train-NormalizedGini:0.058116 
[501]	train-NormalizedGini:0.060364 
[601]	train-NormalizedGini:0.062388 
[701]	train-NormalizedGini:0.061128 
[801]	train-NormalizedGini:0.061777 
[901]	train-NormalizedGini:0.061943 
[1001]	train-NormalizedGini:0.056439 
[1101]	train-NormalizedGini:0.053688 
[1201]	train-NormalizedGini:0.052723 
[1301]	train-NormalizedGini:0.053950 
[1401]	train-NormalizedGini:0.054122 
[1500]	train-NormalizedGini:0.053923 
- Fold10: eta=0.03, max_depth=5, gamma=20, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
+ Fold10: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
[1]	train-NormalizedGini:0.002802 
[101]	train-NormalizedGini:0.088489 
[201]	train-NormalizedGini:0.074354 
[301]	train-NormalizedGini:0.069171 
[401]	train-NormalizedGini:0.069698 
[501]	train-NormalizedGini:0.070132 
[601]	train-NormalizedGini:0.071063 
[701]	train-NormalizedGini:0.069773 
[801]	train-NormalizedGini:0.071058 
[901]	train-NormalizedGini:0.070131 
[1001]	train-NormalizedGini:0.070610 
[1101]	train-NormalizedGini:0.071050 
[1201]	train-NormalizedGini:0.071557 
[1301]	train-NormalizedGini:0.071679 
[1401]	train-NormalizedGini:0.071415 
[1500]	train-NormalizedGini:0.071917 
- Fold10: eta=0.03, max_depth=5, gamma=30, colsample_bytree=1, min_child_weight=120, subsample=0.3, nrounds=1500 
Aggregating results
Selecting tuning parameters
Fitting nrounds = 1500, max_depth = 5, eta = 0.03, gamma = 8, colsample_bytree = 1, min_child_weight = 120, subsample = 0.3 on full training set
[1]	train-NormalizedGini:0.002399 
[101]	train-NormalizedGini:-0.010973 
[201]	train-NormalizedGini:0.004162 
[301]	train-NormalizedGini:-0.004324 
[401]	train-NormalizedGini:-0.002439 
[501]	train-NormalizedGini:-0.002331 
[601]	train-NormalizedGini:0.010290 
[701]	train-NormalizedGini:0.005585 
[801]	train-NormalizedGini:0.004466 
[901]	train-NormalizedGini:0.005802 
[1001]	train-NormalizedGini:0.005691 
[1101]	train-NormalizedGini:0.004813 
[1201]	train-NormalizedGini:0.003648 
[1301]	train-NormalizedGini:-0.000067 
[1401]	train-NormalizedGini:0.000328 
[1500]	train-NormalizedGini:-0.000328 
eXtreme Gradient Boosting 

476753 samples
   185 predictor
     2 classes: 'No', 'Yes' 

No pre-processing
Resampling: Cross-Validated (10 fold) 
Summary of sample sizes: 429078, 429077, 429078, 429078, 429078, 429076, ... 
Resampling results across tuning parameters:

  gamma  nrounds  NormalizedGini
   0      500     0.2812913     
   0     1000     0.2807526     
   0     1500     0.2779780     
   4      500     0.2815048     
   4     1000     0.2808740     
   4     1500     0.2793071     
   8      500     0.2782509     
   8     1000     0.2807110     
   8     1500     0.2816175     
  12      500     0.2733560     
  12     1000     0.2766965     
  12     1500     0.2781161     
  20      500     0.2632380     
  20     1000     0.2661159     
  20     1500     0.2673759     
  30      500     0.2543191     
  30     1000     0.2565275     
  30     1500     0.2578415     

Tuning parameter 'max_depth' was held constant at a value of 5
Tuning parameter 'eta' was held constant at a value of 0.03
Tuning
 parameter 'colsample_bytree' was held constant at a value of 1
Tuning parameter 'min_child_weight' was held constant at a value of 120

Tuning parameter 'subsample' was held constant at a value of 0.3
NormalizedGini was used to select the optimal model using the largest value.
The final values used for the model were nrounds = 1500, max_depth = 5, eta = 0.03, gamma = 8, colsample_bytree = 1, min_child_weight = 120
 and subsample = 0.3.
      user     system    elapsed 
360595.180    299.964  40925.945 
[1]	train-NormalizedGini:0.135816 
[26]	train-NormalizedGini:0.212486 
[51]	train-NormalizedGini:0.228532 
[76]	train-NormalizedGini:0.238266 
[101]	train-NormalizedGini:0.251675 
[126]	train-NormalizedGini:0.259759 
[151]	train-NormalizedGini:0.267042 
[176]	train-NormalizedGini:0.273040 
[201]	train-NormalizedGini:0.276966 
[226]	train-NormalizedGini:0.280764 
[251]	train-NormalizedGini:0.284022 
[276]	train-NormalizedGini:0.286613 
[301]	train-NormalizedGini:0.288191 
[326]	train-NormalizedGini:0.289950 
[351]	train-NormalizedGini:0.291650 
[376]	train-NormalizedGini:0.293008 
[401]	train-NormalizedGini:0.294090 
[426]	train-NormalizedGini:0.294811 
[451]	train-NormalizedGini:0.296096 
[476]	train-NormalizedGini:0.296964 
[501]	train-NormalizedGini:0.297726 
[526]	train-NormalizedGini:0.298278 
[551]	train-NormalizedGini:0.299036 
[576]	train-NormalizedGini:0.299697 
[601]	train-NormalizedGini:0.300322 
[626]	train-NormalizedGini:0.301243 
[651]	train-NormalizedGini:0.301866 
[676]	train-NormalizedGini:0.302320 
[701]	train-NormalizedGini:0.303168 
[726]	train-NormalizedGini:0.303794 
[751]	train-NormalizedGini:0.304424 
[776]	train-NormalizedGini:0.305338 
[801]	train-NormalizedGini:0.305726 
[826]	train-NormalizedGini:0.306169 
[851]	train-NormalizedGini:0.306626 
[876]	train-NormalizedGini:0.307443 
[901]	train-NormalizedGini:0.308221 
[926]	train-NormalizedGini:0.308846 
[951]	train-NormalizedGini:0.309425 
[976]	train-NormalizedGini:0.309623 
[1001]	train-NormalizedGini:0.310065 
[1026]	train-NormalizedGini:0.310711 
[1051]	train-NormalizedGini:0.311282 
[1076]	train-NormalizedGini:0.311643 
[1101]	train-NormalizedGini:0.311961 
[1126]	train-NormalizedGini:0.312375 
[1151]	train-NormalizedGini:0.313085 
[1176]	train-NormalizedGini:0.313374 
[1201]	train-NormalizedGini:0.313658 
[1226]	train-NormalizedGini:0.313856 
[1251]	train-NormalizedGini:0.314281 
[1276]	train-NormalizedGini:0.314570 
[1301]	train-NormalizedGini:0.314835 
[1326]	train-NormalizedGini:0.315640 
[1351]	train-NormalizedGini:0.316007 
[1376]	train-NormalizedGini:0.316710 
[1401]	train-NormalizedGini:0.317132 
[1426]	train-NormalizedGini:0.317290 
[1451]	train-NormalizedGini:0.317596 
[1476]	train-NormalizedGini:0.317952 
[1500]	train-NormalizedGini:0.318404 
    user   system  elapsed 
3599.762    2.952  477.783 
[1] 0.3184043
[1] 0.27442
[1] 595212     59

